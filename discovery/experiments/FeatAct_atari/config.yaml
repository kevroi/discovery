env_name: "ALE/Seaquest-v5"
learner: 'PPO'
project_name: 'PPO_on_Atari_Tests/Seaquest-v5'
timesteps: 100000000
frame_stack: 4
feat_extractor: 'cnn'
activation: 'relu'
feat_dim: 512
policy_type: 'CnnPolicy'
stats_window_size: 10
record_video: False
render_mode: 'rgb_array'


# Shared hypers
lr: 0.0003 # PPO_def: 3e-4
gamma: 0.99 # PPO_def: 0.99
batch_size: 96 # PPO_def: 64, DQN_def: 32

# PPO hypers & SB3 defaults
n_epochs: 4 # 10
n_steps: 128 # 2048
n_envs: 8 # 1 #TODO this affects DDQN too!

# (D)DQN hypers & SB3 defaults
learning_starts: 1000 # 100
train_freq: 1 # 4
exploration_final_eps: 0.1 # 0.05
target_update_interval: 500 # 10000