{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import torch\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from discovery.utils.feat_extractors import NatureCNN\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "from discovery.experiments.FeatAct_minigrid.helpers import pre_process_obs\n",
    "import cv2\n",
    "\n",
    "from discovery.utils import filesys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/kevinroice/Documents/research/discovery/discovery/experiments/FeatAct_atari/models/Seaquest-v5_mpqgvvr1.zip.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/kevinroice/Documents/research/discovery/discovery/experiments/FeatAct_atari/models/Seaquest-v5_mpqgvvr1.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/discovery/venv/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:679\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    677\u001b[0m     get_system_info()\n\u001b[0;32m--> 679\u001b[0m data, params, pytorch_variables \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_zip_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_system_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo params found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Code/discovery/venv/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:390\u001b[0m, in \u001b[0;36mload_from_zip_file\u001b[0;34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_zip_file\u001b[39m(\n\u001b[1;32m    364\u001b[0m     load_path: Union[\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath, io\u001b[38;5;241m.\u001b[39mBufferedIOBase],\n\u001b[1;32m    365\u001b[0m     load_data: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m     print_system_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    370\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]], TensorDict, Optional[TensorDict]]:\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    Load model data from a .zip archive\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m        and dict of pytorch variables\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     load_path \u001b[38;5;241m=\u001b[39m \u001b[43mopen_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     device \u001b[38;5;241m=\u001b[39m get_device(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/discovery/venv/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:234\u001b[0m, in \u001b[0;36mopen_path_str\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;129m@open_path\u001b[39m\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_path_str\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m, verbose: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, suffix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m io\u001b[38;5;241m.\u001b[39mBufferedIOBase:\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    Open a path given by a string. If writing to the path, the function ensures\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    that the path exists.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/discovery/venv/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:286\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    279\u001b[0m         path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# if opening was successful uses the identity function\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m#   with corrections\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/discovery/venv/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:266\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    264\u001b[0m             path, suffix \u001b[38;5;241m=\u001b[39m newpath, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/discovery/venv/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:258\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m suffix \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pathlib.py:1044\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1043\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/kevinroice/Documents/research/discovery/discovery/experiments/FeatAct_atari/models/Seaquest-v5_mpqgvvr1.zip.zip'"
     ]
    }
   ],
   "source": [
    "filesys.set_directory_in_project()\n",
    "agent = PPO.load(\"discovery/experiments/FeatAct_atari/models/Seaquest-v5_mpqgvvr1.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the seaquest dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = f\"/Users/kevinroice/Documents/research/discovery/datasets/AAD/clean/SeaquestNoFrameskip-v4/episode(1).hdf5\"\n",
    "with h5py.File(path, \"r\") as f:\n",
    "    state = f[\"state\"][...]\n",
    "labels = np.load(\"/Users/kevinroice/Documents/research/discovery/datasets/AAD/clean/SeaquestNoFrameskip-v4/episode(1)_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_feats(model, obss):\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for obs in obss:\n",
    "            obs = pre_process_obs(obs[0], model)\n",
    "            # print(obs[0].shape)\n",
    "            if model.__class__.__name__ == \"DoubleDQN\":\n",
    "                x = model.policy.extract_features(obs, model.policy.q_net.features_extractor)\n",
    "            elif model.__class__.__name__ == \"PPO\":\n",
    "                x = model.policy.extract_features(obs)\n",
    "            feats.append(x)  \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state[0, :, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_atari(dataset: np.ndarray):\n",
    "    num_images = dataset.shape[0]\n",
    "    preprocessed_images = np.zeros((num_images, 84, 84), dtype=np.uint8)\n",
    "\n",
    "    for i in range(num_images):\n",
    "        preprocessed_images[i] = cv2.resize(cv2.cvtColor(dataset[i], cv2.COLOR_RGB2GRAY), (84, 84))\n",
    "\n",
    "        # Stack frames\n",
    "        stacked_images = np.zeros((num_images - 3, 4, 84, 84), dtype=np.uint8)\n",
    "        for i in range(num_images - 3):\n",
    "            stacked_images[i] = np.stack(\n",
    "                [\n",
    "                    preprocessed_images[i],\n",
    "                    preprocessed_images[i + 1],\n",
    "                    preprocessed_images[i + 2],\n",
    "                    preprocessed_images[i + 3],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return stacked_images\n",
    "\n",
    "\n",
    "def stack_labels(labels):\n",
    "    stacked_labels = np.zeros((labels.shape[0] - 3, 1), dtype=np.uint8)\n",
    "    for i in range(labels.shape[0] - 3):\n",
    "        stacked_labels[i] = labels[i + 3]\n",
    "    return stacked_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_to_feats_atari(model, preprocs):\n",
    "    with torch.no_grad():\n",
    "        tensors = obs_as_tensor(preprocs, model.device)\n",
    "        print(tensors.shape)\n",
    "        feats = model.policy.extract_features(tensors) \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_states = pre_process_atari(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_labels = stack_labels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1354, 4, 84, 84)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processed_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1354, 1)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn all numbers into 0 and 2s into 1\n",
    "stacked_labels[stacked_labels == 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(stacked_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1354, 4, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "feats = preproc_to_feats_atari(agent, pre_processed_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discovery.utils import sg_detection \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import importlib\n",
    "\n",
    "def train_classifier(clf, X, labels,\n",
    "                     n_epochs=500,\n",
    "                     batch_size=32,\n",
    "                     test_size=0.2, random_state=0):\n",
    "    # X = torch.cat(feats, dim=0)\n",
    "    y = torch.tensor(labels).float().squeeze()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=random_state)\n",
    "    \n",
    "    best_acc = - np.inf\n",
    "    best_weights = None\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size) # TODO: check if the last batch is included\n",
    "    loss_fn = nn.BCELoss(reduction='none')  # reduction='none' to get per-sample loss, not mean\n",
    "\n",
    "    num_pos = y_train.sum()\n",
    "    num_neg = len(y_train) - num_pos\n",
    "    base_weight = torch.tensor([1.0, num_neg/num_pos]) # for weighted mean in loss calculation\n",
    "    \n",
    "    optimizer = optim.Adam(clf.parameters(), lr=0.0001)\n",
    "    # TODO: collect positive examples, and concatenate them to each batch\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        clf.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=False) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = clf(X_batch)\n",
    "                y_batch = y_batch.unsqueeze(1)\n",
    "                weight = torch.where(y_batch == 1, base_weight[1], base_weight[0])\n",
    "                loss2 = loss_fn(y_pred, y_batch)\n",
    "                final_loss = torch.mean(weight*loss2)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                final_loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(final_loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # # evaluate accuracy at end of each epoch\n",
    "        # clf.eval()\n",
    "        # y_pred = clf(X_test)\n",
    "        # acc = (y_pred.round() == y_test).float().mean()\n",
    "        # acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(clf.state_dict())\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discovery.utils import sg_detection \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import importlib\n",
    "\n",
    "def calculate_class_weights(labels, n_classes):\n",
    "    class_counts = torch.bincount(labels.squeeze(), minlength=n_classes)\n",
    "    class_weights = 1. / class_counts.float()\n",
    "    class_weights /= class_weights.sum()  # normalize the weights\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "def train_classifier(clf, X, labels, n_classes,\n",
    "                     n_epochs=500, batch_size=32,\n",
    "                     test_size=0.2, random_state=0):\n",
    "    # X = torch.cat(feats, dim=0)\n",
    "    y = torch.tensor(labels).long() # ensure labels are integers\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=random_state)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    print(type(y_train[0]), y_train[0], y_train[0].shape)\n",
    "    class_weights = calculate_class_weights(y_train, n_classes)\n",
    "    \n",
    "    best_acc = - np.inf\n",
    "    best_weights = None\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)  # Check if the last batch is included\n",
    "    print(class_weights)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)  # Use class weights in the loss function\n",
    "    \n",
    "    optimizer = optim.Adam(clf.parameters(), lr=0.0001)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        clf.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=False) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # Take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # Ensure y_batch is of type LongTensor\n",
    "                y_batch = y_batch.long()\n",
    "                # Forward pass\n",
    "                y_pred = clf(X_batch)\n",
    "                # Ensure y_pred is of type FloatTensor\n",
    "                y_pred = y_pred.type(torch.FloatTensor)\n",
    "                y_batch = y_batch.type(torch.FloatTensor)\n",
    "                print(type(y_pred), y_pred.shape)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                # Print progress\n",
    "                _, y_pred_labels = torch.max(y_pred, 1)\n",
    "                acc = (y_pred_labels == y_batch).float().mean()\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "        # Evaluate accuracy at end of each epoch\n",
    "        clf.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = clf(X_test)\n",
    "            _, y_pred_labels = torch.max(y_pred, 1)\n",
    "            acc = (y_pred_labels == y_test).float().mean()\n",
    "            acc = float(acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(clf.state_dict())\n",
    "    \n",
    "    clf.load_state_dict(best_weights)\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=uint8)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 34/34 [00:00<00:00, 114.22batch/s, acc=0.63, loss=1.36]  \n",
      "Epoch 1: 100%|██████████| 34/34 [00:00<00:00, 93.60batch/s, acc=0.667, loss=1.2]   \n",
      "Epoch 2: 100%|██████████| 34/34 [00:00<00:00, 196.03batch/s, acc=0.704, loss=1.04] \n",
      "Epoch 3: 100%|██████████| 34/34 [00:00<00:00, 269.14batch/s, acc=0.852, loss=0.908]\n",
      "Epoch 4: 100%|██████████| 34/34 [00:00<00:00, 259.00batch/s, acc=0.852, loss=0.803]\n",
      "Epoch 5: 100%|██████████| 34/34 [00:00<00:00, 242.62batch/s, acc=0.852, loss=0.723]\n",
      "Epoch 6: 100%|██████████| 34/34 [00:00<00:00, 257.02batch/s, acc=0.852, loss=0.658]\n",
      "Epoch 7: 100%|██████████| 34/34 [00:00<00:00, 227.32batch/s, acc=0.852, loss=0.605]\n",
      "Epoch 8: 100%|██████████| 34/34 [00:00<00:00, 157.23batch/s, acc=0.889, loss=0.56] \n",
      "Epoch 9: 100%|██████████| 34/34 [00:00<00:00, 240.80batch/s, acc=0.889, loss=0.521]\n",
      "Epoch 10: 100%|██████████| 34/34 [00:00<00:00, 232.28batch/s, acc=0.889, loss=0.488]\n",
      "Epoch 11: 100%|██████████| 34/34 [00:00<00:00, 243.32batch/s, acc=0.926, loss=0.459]\n",
      "Epoch 12: 100%|██████████| 34/34 [00:00<00:00, 236.96batch/s, acc=0.926, loss=0.433]\n",
      "Epoch 13: 100%|██████████| 34/34 [00:00<00:00, 208.52batch/s, acc=0.963, loss=0.41] \n",
      "Epoch 14: 100%|██████████| 34/34 [00:00<00:00, 220.43batch/s, acc=0.963, loss=0.39] \n",
      "Epoch 15: 100%|██████████| 34/34 [00:00<00:00, 209.63batch/s, acc=0.963, loss=0.371]\n",
      "Epoch 16: 100%|██████████| 34/34 [00:00<00:00, 245.62batch/s, acc=0.963, loss=0.355]\n",
      "Epoch 17: 100%|██████████| 34/34 [00:00<00:00, 221.66batch/s, acc=0.963, loss=0.34] \n",
      "Epoch 18: 100%|██████████| 34/34 [00:00<00:00, 164.91batch/s, acc=0.926, loss=0.326]\n",
      "Epoch 19: 100%|██████████| 34/34 [00:00<00:00, 74.05batch/s, acc=0.926, loss=0.314] \n",
      "Epoch 20: 100%|██████████| 34/34 [00:00<00:00, 126.38batch/s, acc=0.926, loss=0.302]\n",
      "Epoch 21: 100%|██████████| 34/34 [00:00<00:00, 147.12batch/s, acc=0.963, loss=0.292]\n",
      "Epoch 22: 100%|██████████| 34/34 [00:00<00:00, 203.82batch/s, acc=0.963, loss=0.282]\n",
      "Epoch 23: 100%|██████████| 34/34 [00:00<00:00, 254.64batch/s, acc=0.963, loss=0.273]\n",
      "Epoch 24: 100%|██████████| 34/34 [00:00<00:00, 253.52batch/s, acc=0.963, loss=0.265]\n",
      "Epoch 25: 100%|██████████| 34/34 [00:00<00:00, 229.46batch/s, acc=0.963, loss=0.257]\n",
      "Epoch 26: 100%|██████████| 34/34 [00:00<00:00, 134.27batch/s, acc=0.963, loss=0.249]\n",
      "Epoch 27: 100%|██████████| 34/34 [00:00<00:00, 120.90batch/s, acc=0.963, loss=0.243]\n",
      "Epoch 28: 100%|██████████| 34/34 [00:00<00:00, 140.12batch/s, acc=0.963, loss=0.236]\n",
      "Epoch 29: 100%|██████████| 34/34 [00:00<00:00, 91.20batch/s, acc=0.963, loss=0.23]  \n",
      "Epoch 30: 100%|██████████| 34/34 [00:00<00:00, 181.28batch/s, acc=0.963, loss=0.225]\n",
      "Epoch 31: 100%|██████████| 34/34 [00:00<00:00, 206.25batch/s, acc=0.963, loss=0.219]\n",
      "Epoch 32: 100%|██████████| 34/34 [00:00<00:00, 228.23batch/s, acc=0.963, loss=0.214]\n",
      "Epoch 33: 100%|██████████| 34/34 [00:00<00:00, 245.85batch/s, acc=0.963, loss=0.209]\n",
      "Epoch 34: 100%|██████████| 34/34 [00:00<00:00, 248.69batch/s, acc=0.963, loss=0.205]\n",
      "Epoch 35: 100%|██████████| 34/34 [00:00<00:00, 246.42batch/s, acc=0.963, loss=0.2]  \n",
      "Epoch 36: 100%|██████████| 34/34 [00:00<00:00, 222.29batch/s, acc=0.963, loss=0.196]\n",
      "Epoch 37: 100%|██████████| 34/34 [00:00<00:00, 150.75batch/s, acc=0.963, loss=0.192]\n",
      "Epoch 38: 100%|██████████| 34/34 [00:00<00:00, 130.28batch/s, acc=0.963, loss=0.188]\n",
      "Epoch 39: 100%|██████████| 34/34 [00:00<00:00, 155.23batch/s, acc=0.963, loss=0.184]\n",
      "Epoch 40: 100%|██████████| 34/34 [00:00<00:00, 83.31batch/s, acc=0.963, loss=0.181] \n",
      "Epoch 41: 100%|██████████| 34/34 [00:00<00:00, 144.08batch/s, acc=0.963, loss=0.177]\n",
      "Epoch 42: 100%|██████████| 34/34 [00:00<00:00, 187.77batch/s, acc=0.963, loss=0.174]\n",
      "Epoch 43: 100%|██████████| 34/34 [00:00<00:00, 224.07batch/s, acc=0.963, loss=0.171]\n",
      "Epoch 44: 100%|██████████| 34/34 [00:00<00:00, 200.63batch/s, acc=0.963, loss=0.168]\n",
      "Epoch 45: 100%|██████████| 34/34 [00:00<00:00, 157.56batch/s, acc=0.963, loss=0.165]\n",
      "Epoch 46: 100%|██████████| 34/34 [00:00<00:00, 82.43batch/s, acc=0.963, loss=0.162] \n",
      "Epoch 47: 100%|██████████| 34/34 [00:00<00:00, 80.35batch/s, acc=0.963, loss=0.159] \n",
      "Epoch 48: 100%|██████████| 34/34 [00:00<00:00, 65.66batch/s, acc=0.963, loss=0.156] \n",
      "Epoch 49: 100%|██████████| 34/34 [00:00<00:00, 168.11batch/s, acc=0.963, loss=0.154]\n",
      "Epoch 50: 100%|██████████| 34/34 [00:00<00:00, 182.40batch/s, acc=0.963, loss=0.151]\n",
      "Epoch 51: 100%|██████████| 34/34 [00:00<00:00, 192.32batch/s, acc=0.963, loss=0.148]\n",
      "Epoch 52: 100%|██████████| 34/34 [00:00<00:00, 117.64batch/s, acc=0.963, loss=0.146]\n",
      "Epoch 53: 100%|██████████| 34/34 [00:00<00:00, 170.10batch/s, acc=0.963, loss=0.144]\n",
      "Epoch 54: 100%|██████████| 34/34 [00:00<00:00, 129.88batch/s, acc=0.963, loss=0.141]\n",
      "Epoch 55: 100%|██████████| 34/34 [00:00<00:00, 88.57batch/s, acc=0.963, loss=0.139] \n",
      "Epoch 56: 100%|██████████| 34/34 [00:00<00:00, 60.60batch/s, acc=0.963, loss=0.137] \n",
      "Epoch 57: 100%|██████████| 34/34 [00:00<00:00, 59.66batch/s, acc=0.963, loss=0.135]\n",
      "Epoch 58: 100%|██████████| 34/34 [00:00<00:00, 104.75batch/s, acc=0.963, loss=0.133]\n",
      "Epoch 59: 100%|██████████| 34/34 [00:00<00:00, 139.41batch/s, acc=0.963, loss=0.131]\n",
      "Epoch 60: 100%|██████████| 34/34 [00:00<00:00, 129.26batch/s, acc=0.963, loss=0.129]\n",
      "Epoch 61: 100%|██████████| 34/34 [00:00<00:00, 142.80batch/s, acc=0.963, loss=0.127]\n",
      "Epoch 62: 100%|██████████| 34/34 [00:00<00:00, 92.81batch/s, acc=0.963, loss=0.125] \n",
      "Epoch 63: 100%|██████████| 34/34 [00:00<00:00, 78.00batch/s, acc=0.963, loss=0.123] \n",
      "Epoch 64: 100%|██████████| 34/34 [00:00<00:00, 145.46batch/s, acc=0.963, loss=0.121]\n",
      "Epoch 65: 100%|██████████| 34/34 [00:00<00:00, 112.33batch/s, acc=0.963, loss=0.12] \n",
      "Epoch 66: 100%|██████████| 34/34 [00:00<00:00, 123.70batch/s, acc=0.963, loss=0.118]\n",
      "Epoch 67: 100%|██████████| 34/34 [00:00<00:00, 126.81batch/s, acc=0.963, loss=0.116]\n",
      "Epoch 68: 100%|██████████| 34/34 [00:00<00:00, 77.56batch/s, acc=1, loss=0.115]     \n",
      "Epoch 69: 100%|██████████| 34/34 [00:00<00:00, 51.59batch/s, acc=1, loss=0.113]    \n",
      "Epoch 70: 100%|██████████| 34/34 [00:00<00:00, 61.11batch/s, acc=1, loss=0.111]    \n",
      "Epoch 71: 100%|██████████| 34/34 [00:00<00:00, 95.14batch/s, acc=1, loss=0.11]      \n",
      "Epoch 72: 100%|██████████| 34/34 [00:00<00:00, 128.96batch/s, acc=1, loss=0.108]    \n",
      "Epoch 73: 100%|██████████| 34/34 [00:00<00:00, 105.73batch/s, acc=1, loss=0.107]    \n",
      "Epoch 74: 100%|██████████| 34/34 [00:00<00:00, 74.92batch/s, acc=1, loss=0.106]     \n",
      "Epoch 75: 100%|██████████| 34/34 [00:00<00:00, 107.89batch/s, acc=1, loss=0.104]    \n",
      "Epoch 76: 100%|██████████| 34/34 [00:00<00:00, 72.52batch/s, acc=1, loss=0.103]     \n",
      "Epoch 77: 100%|██████████| 34/34 [00:00<00:00, 97.76batch/s, acc=1, loss=0.101]     \n",
      "Epoch 78: 100%|██████████| 34/34 [00:00<00:00, 92.40batch/s, acc=1, loss=0.1]       \n",
      "Epoch 79: 100%|██████████| 34/34 [00:00<00:00, 127.63batch/s, acc=1, loss=0.0988]   \n",
      "Epoch 80: 100%|██████████| 34/34 [00:00<00:00, 164.00batch/s, acc=1, loss=0.0975]   \n",
      "Epoch 81: 100%|██████████| 34/34 [00:00<00:00, 123.51batch/s, acc=1, loss=0.0963]   \n",
      "Epoch 82: 100%|██████████| 34/34 [00:00<00:00, 81.10batch/s, acc=1, loss=0.095]     \n",
      "Epoch 83: 100%|██████████| 34/34 [00:00<00:00, 134.34batch/s, acc=1, loss=0.0938]   \n",
      "Epoch 84: 100%|██████████| 34/34 [00:00<00:00, 159.10batch/s, acc=1, loss=0.0927]   \n",
      "Epoch 85: 100%|██████████| 34/34 [00:00<00:00, 154.20batch/s, acc=1, loss=0.0915]   \n",
      "Epoch 86: 100%|██████████| 34/34 [00:00<00:00, 111.11batch/s, acc=1, loss=0.0904]   \n",
      "Epoch 87: 100%|██████████| 34/34 [00:00<00:00, 136.08batch/s, acc=1, loss=0.0893]   \n",
      "Epoch 88: 100%|██████████| 34/34 [00:00<00:00, 65.80batch/s, acc=1, loss=0.0882]    \n",
      "Epoch 89: 100%|██████████| 34/34 [00:00<00:00, 103.32batch/s, acc=1, loss=0.0871]   \n",
      "Epoch 90: 100%|██████████| 34/34 [00:00<00:00, 70.84batch/s, acc=1, loss=0.086]     \n",
      "Epoch 91: 100%|██████████| 34/34 [00:00<00:00, 121.83batch/s, acc=1, loss=0.085]    \n",
      "Epoch 92: 100%|██████████| 34/34 [00:00<00:00, 86.50batch/s, acc=1, loss=0.084]     \n",
      "Epoch 93: 100%|██████████| 34/34 [00:00<00:00, 106.54batch/s, acc=1, loss=0.083]    \n",
      "Epoch 94: 100%|██████████| 34/34 [00:00<00:00, 78.32batch/s, acc=1, loss=0.082]     \n",
      "Epoch 95: 100%|██████████| 34/34 [00:00<00:00, 79.85batch/s, acc=1, loss=0.0811]    \n",
      "Epoch 96: 100%|██████████| 34/34 [00:00<00:00, 99.51batch/s, acc=1, loss=0.0801]    \n",
      "Epoch 97: 100%|██████████| 34/34 [00:00<00:00, 91.44batch/s, acc=1, loss=0.0792]    \n",
      "Epoch 98: 100%|██████████| 34/34 [00:00<00:00, 124.88batch/s, acc=1, loss=0.0783]   \n",
      "Epoch 99: 100%|██████████| 34/34 [00:00<00:00, 62.76batch/s, acc=1, loss=0.0774]    \n",
      "Epoch 100: 100%|██████████| 34/34 [00:00<00:00, 97.02batch/s, acc=1, loss=0.0765]    \n",
      "Epoch 101: 100%|██████████| 34/34 [00:00<00:00, 83.07batch/s, acc=1, loss=0.0756]    \n",
      "Epoch 102: 100%|██████████| 34/34 [00:00<00:00, 68.07batch/s, acc=1, loss=0.0748]    \n",
      "Epoch 103: 100%|██████████| 34/34 [00:00<00:00, 81.38batch/s, acc=1, loss=0.0739]    \n",
      "Epoch 104: 100%|██████████| 34/34 [00:00<00:00, 69.89batch/s, acc=1, loss=0.0731]    \n",
      "Epoch 105: 100%|██████████| 34/34 [00:00<00:00, 124.44batch/s, acc=1, loss=0.0723]   \n",
      "Epoch 106: 100%|██████████| 34/34 [00:00<00:00, 93.26batch/s, acc=1, loss=0.0715]    \n",
      "Epoch 107: 100%|██████████| 34/34 [00:00<00:00, 131.07batch/s, acc=1, loss=0.0707]   \n",
      "Epoch 108: 100%|██████████| 34/34 [00:00<00:00, 66.60batch/s, acc=1, loss=0.0699]    \n",
      "Epoch 109: 100%|██████████| 34/34 [00:00<00:00, 72.83batch/s, acc=1, loss=0.0692]    \n",
      "Epoch 110: 100%|██████████| 34/34 [00:00<00:00, 78.71batch/s, acc=1, loss=0.0684]    \n",
      "Epoch 111: 100%|██████████| 34/34 [00:00<00:00, 94.73batch/s, acc=1, loss=0.0677]    \n",
      "Epoch 112: 100%|██████████| 34/34 [00:00<00:00, 104.98batch/s, acc=1, loss=0.067]    \n",
      "Epoch 113: 100%|██████████| 34/34 [00:00<00:00, 95.34batch/s, acc=1, loss=0.0663]    \n",
      "Epoch 114: 100%|██████████| 34/34 [00:00<00:00, 68.43batch/s, acc=1, loss=0.0656]    \n",
      "Epoch 115: 100%|██████████| 34/34 [00:00<00:00, 130.86batch/s, acc=1, loss=0.0649]   \n",
      "Epoch 116: 100%|██████████| 34/34 [00:00<00:00, 86.28batch/s, acc=1, loss=0.0642]    \n",
      "Epoch 117: 100%|██████████| 34/34 [00:00<00:00, 118.06batch/s, acc=1, loss=0.0635]   \n",
      "Epoch 118: 100%|██████████| 34/34 [00:00<00:00, 124.96batch/s, acc=1, loss=0.0628]   \n",
      "Epoch 119: 100%|██████████| 34/34 [00:00<00:00, 54.79batch/s, acc=1, loss=0.0622]    \n",
      "Epoch 120: 100%|██████████| 34/34 [00:00<00:00, 131.15batch/s, acc=1, loss=0.0615]   \n",
      "Epoch 121: 100%|██████████| 34/34 [00:00<00:00, 81.04batch/s, acc=1, loss=0.0609]    \n",
      "Epoch 122: 100%|██████████| 34/34 [00:00<00:00, 96.86batch/s, acc=1, loss=0.0603]    \n",
      "Epoch 123: 100%|██████████| 34/34 [00:00<00:00, 85.80batch/s, acc=1, loss=0.0597]    \n",
      "Epoch 124: 100%|██████████| 34/34 [00:00<00:00, 62.53batch/s, acc=1, loss=0.0591]    \n",
      "Epoch 125: 100%|██████████| 34/34 [00:00<00:00, 72.56batch/s, acc=1, loss=0.0585]   \n",
      "Epoch 126: 100%|██████████| 34/34 [00:00<00:00, 92.89batch/s, acc=1, loss=0.0579]    \n",
      "Epoch 127: 100%|██████████| 34/34 [00:00<00:00, 84.31batch/s, acc=1, loss=0.0573]     \n",
      "Epoch 128: 100%|██████████| 34/34 [00:00<00:00, 73.93batch/s, acc=1, loss=0.0567]     \n",
      "Epoch 129: 100%|██████████| 34/34 [00:00<00:00, 66.31batch/s, acc=1, loss=0.0561]    \n",
      "Epoch 130: 100%|██████████| 34/34 [00:00<00:00, 172.63batch/s, acc=1, loss=0.0556]    \n",
      "Epoch 131: 100%|██████████| 34/34 [00:00<00:00, 148.34batch/s, acc=1, loss=0.055]     \n",
      "Epoch 132: 100%|██████████| 34/34 [00:00<00:00, 182.51batch/s, acc=1, loss=0.0545]    \n",
      "Epoch 133: 100%|██████████| 34/34 [00:00<00:00, 108.14batch/s, acc=1, loss=0.0539]    \n",
      "Epoch 134: 100%|██████████| 34/34 [00:00<00:00, 122.69batch/s, acc=1, loss=0.0534]    \n",
      "Epoch 135: 100%|██████████| 34/34 [00:00<00:00, 167.38batch/s, acc=1, loss=0.0529]    \n",
      "Epoch 136: 100%|██████████| 34/34 [00:00<00:00, 179.95batch/s, acc=1, loss=0.0524]    \n",
      "Epoch 137: 100%|██████████| 34/34 [00:00<00:00, 185.75batch/s, acc=1, loss=0.0519]    \n",
      "Epoch 138: 100%|██████████| 34/34 [00:00<00:00, 85.35batch/s, acc=1, loss=0.0514]    \n",
      "Epoch 139: 100%|██████████| 34/34 [00:00<00:00, 211.56batch/s, acc=1, loss=0.0509]    \n",
      "Epoch 140: 100%|██████████| 34/34 [00:00<00:00, 233.60batch/s, acc=1, loss=0.0504]    \n",
      "Epoch 141: 100%|██████████| 34/34 [00:00<00:00, 214.62batch/s, acc=1, loss=0.0499]    \n",
      "Epoch 142: 100%|██████████| 34/34 [00:00<00:00, 194.57batch/s, acc=1, loss=0.0494]    \n",
      "Epoch 143: 100%|██████████| 34/34 [00:00<00:00, 71.57batch/s, acc=1, loss=0.0489]    \n",
      "Epoch 144: 100%|██████████| 34/34 [00:00<00:00, 106.60batch/s, acc=1, loss=0.0485]    \n",
      "Epoch 145: 100%|██████████| 34/34 [00:00<00:00, 144.65batch/s, acc=1, loss=0.048]     \n",
      "Epoch 146: 100%|██████████| 34/34 [00:00<00:00, 136.18batch/s, acc=1, loss=0.0476]    \n",
      "Epoch 147: 100%|██████████| 34/34 [00:00<00:00, 78.29batch/s, acc=1, loss=0.0471]     \n",
      "Epoch 148: 100%|██████████| 34/34 [00:00<00:00, 147.36batch/s, acc=1, loss=0.0467]    \n",
      "Epoch 149: 100%|██████████| 34/34 [00:00<00:00, 95.56batch/s, acc=1, loss=0.0462]     \n",
      "Epoch 150: 100%|██████████| 34/34 [00:00<00:00, 222.71batch/s, acc=1, loss=0.0458]    \n",
      "Epoch 151: 100%|██████████| 34/34 [00:00<00:00, 182.28batch/s, acc=1, loss=0.0454]    \n",
      "Epoch 152: 100%|██████████| 34/34 [00:00<00:00, 88.90batch/s, acc=1, loss=0.0449]     \n",
      "Epoch 153: 100%|██████████| 34/34 [00:00<00:00, 108.32batch/s, acc=1, loss=0.0445]    \n",
      "Epoch 154: 100%|██████████| 34/34 [00:00<00:00, 104.95batch/s, acc=1, loss=0.0441]   \n",
      "Epoch 155: 100%|██████████| 34/34 [00:00<00:00, 160.99batch/s, acc=1, loss=0.0437]    \n",
      "Epoch 156: 100%|██████████| 34/34 [00:00<00:00, 58.49batch/s, acc=1, loss=0.0433]    \n",
      "Epoch 157: 100%|██████████| 34/34 [00:00<00:00, 144.37batch/s, acc=1, loss=0.0429]    \n",
      "Epoch 158: 100%|██████████| 34/34 [00:00<00:00, 102.34batch/s, acc=1, loss=0.0425]    \n",
      "Epoch 159: 100%|██████████| 34/34 [00:00<00:00, 92.02batch/s, acc=1, loss=0.0421]    \n",
      "Epoch 160: 100%|██████████| 34/34 [00:00<00:00, 52.35batch/s, acc=1, loss=0.0417]    \n",
      "Epoch 161: 100%|██████████| 34/34 [00:00<00:00, 74.08batch/s, acc=1, loss=0.0413]     \n",
      "Epoch 162: 100%|██████████| 34/34 [00:00<00:00, 118.35batch/s, acc=1, loss=0.041]     \n",
      "Epoch 163: 100%|██████████| 34/34 [00:00<00:00, 94.05batch/s, acc=1, loss=0.0406]     \n",
      "Epoch 164: 100%|██████████| 34/34 [00:00<00:00, 54.53batch/s, acc=1, loss=0.0402]    \n",
      "Epoch 165: 100%|██████████| 34/34 [00:00<00:00, 78.19batch/s, acc=1, loss=0.0399]     \n",
      "Epoch 166: 100%|██████████| 34/34 [00:00<00:00, 83.43batch/s, acc=1, loss=0.0395]    \n",
      "Epoch 167: 100%|██████████| 34/34 [00:00<00:00, 47.72batch/s, acc=1, loss=0.0392]    \n",
      "Epoch 168: 100%|██████████| 34/34 [00:00<00:00, 58.65batch/s, acc=1, loss=0.0388]    \n",
      "Epoch 169: 100%|██████████| 34/34 [00:00<00:00, 82.09batch/s, acc=1, loss=0.0385]     \n",
      "Epoch 170: 100%|██████████| 34/34 [00:00<00:00, 69.29batch/s, acc=1, loss=0.0381]    \n",
      "Epoch 171: 100%|██████████| 34/34 [00:00<00:00, 38.56batch/s, acc=1, loss=0.0378]    \n",
      "Epoch 172: 100%|██████████| 34/34 [00:00<00:00, 56.13batch/s, acc=1, loss=0.0374]    \n",
      "Epoch 173: 100%|██████████| 34/34 [00:00<00:00, 69.91batch/s, acc=1, loss=0.0371]    \n",
      "Epoch 174: 100%|██████████| 34/34 [00:00<00:00, 41.39batch/s, acc=1, loss=0.0368]    \n",
      "Epoch 175: 100%|██████████| 34/34 [00:00<00:00, 59.30batch/s, acc=1, loss=0.0364]    \n",
      "Epoch 176: 100%|██████████| 34/34 [00:00<00:00, 70.95batch/s, acc=1, loss=0.0361]     \n",
      "Epoch 177: 100%|██████████| 34/34 [00:00<00:00, 87.04batch/s, acc=1, loss=0.0358]    \n",
      "Epoch 178: 100%|██████████| 34/34 [00:00<00:00, 56.36batch/s, acc=1, loss=0.0355]    \n",
      "Epoch 179: 100%|██████████| 34/34 [00:00<00:00, 37.10batch/s, acc=1, loss=0.0352]    \n",
      "Epoch 180: 100%|██████████| 34/34 [00:00<00:00, 61.24batch/s, acc=1, loss=0.0349]    \n",
      "Epoch 181: 100%|██████████| 34/34 [00:00<00:00, 57.68batch/s, acc=1, loss=0.0346]    \n",
      "Epoch 182: 100%|██████████| 34/34 [00:00<00:00, 71.30batch/s, acc=1, loss=0.0343]     \n",
      "Epoch 183: 100%|██████████| 34/34 [00:00<00:00, 80.44batch/s, acc=1, loss=0.034]     \n",
      "Epoch 184: 100%|██████████| 34/34 [00:00<00:00, 66.53batch/s, acc=1, loss=0.0337]     \n",
      "Epoch 185: 100%|██████████| 34/34 [00:00<00:00, 68.79batch/s, acc=1, loss=0.0334]    \n",
      "Epoch 186: 100%|██████████| 34/34 [00:00<00:00, 102.24batch/s, acc=1, loss=0.0331]    \n",
      "Epoch 187: 100%|██████████| 34/34 [00:00<00:00, 65.80batch/s, acc=1, loss=0.0328]     \n",
      "Epoch 188: 100%|██████████| 34/34 [00:00<00:00, 50.73batch/s, acc=1, loss=0.0325]    \n",
      "Epoch 189: 100%|██████████| 34/34 [00:00<00:00, 57.76batch/s, acc=1, loss=0.0322]    \n",
      "Epoch 190: 100%|██████████| 34/34 [00:00<00:00, 92.57batch/s, acc=1, loss=0.032]     \n",
      "Epoch 191: 100%|██████████| 34/34 [00:00<00:00, 111.52batch/s, acc=1, loss=0.0317]    \n",
      "Epoch 192: 100%|██████████| 34/34 [00:00<00:00, 59.06batch/s, acc=1, loss=0.0314]     \n",
      "Epoch 193: 100%|██████████| 34/34 [00:00<00:00, 137.36batch/s, acc=1, loss=0.0312]    \n",
      "Epoch 194: 100%|██████████| 34/34 [00:00<00:00, 145.23batch/s, acc=1, loss=0.0309]    \n",
      "Epoch 195: 100%|██████████| 34/34 [00:00<00:00, 83.06batch/s, acc=1, loss=0.0306]     \n",
      "Epoch 196: 100%|██████████| 34/34 [00:00<00:00, 91.53batch/s, acc=1, loss=0.0304]     \n",
      "Epoch 197: 100%|██████████| 34/34 [00:00<00:00, 125.54batch/s, acc=1, loss=0.0301]    \n",
      "Epoch 198: 100%|██████████| 34/34 [00:00<00:00, 111.95batch/s, acc=1, loss=0.0299]    \n",
      "Epoch 199: 100%|██████████| 34/34 [00:00<00:00, 102.33batch/s, acc=1, loss=0.0296]    \n",
      "Epoch 200: 100%|██████████| 34/34 [00:00<00:00, 118.19batch/s, acc=1, loss=0.0294]    \n",
      "Epoch 201: 100%|██████████| 34/34 [00:00<00:00, 181.52batch/s, acc=1, loss=0.0291]    \n",
      "Epoch 202: 100%|██████████| 34/34 [00:00<00:00, 71.72batch/s, acc=1, loss=0.0289]     \n",
      "Epoch 203: 100%|██████████| 34/34 [00:00<00:00, 114.27batch/s, acc=1, loss=0.0286]     \n",
      "Epoch 204: 100%|██████████| 34/34 [00:00<00:00, 151.07batch/s, acc=1, loss=0.0284]     \n",
      "Epoch 205: 100%|██████████| 34/34 [00:00<00:00, 116.80batch/s, acc=1, loss=0.0282]     \n",
      "Epoch 206: 100%|██████████| 34/34 [00:00<00:00, 120.62batch/s, acc=1, loss=0.0279]     \n",
      "Epoch 207: 100%|██████████| 34/34 [00:00<00:00, 164.58batch/s, acc=1, loss=0.0277]     \n",
      "Epoch 208: 100%|██████████| 34/34 [00:00<00:00, 181.73batch/s, acc=1, loss=0.0275]     \n",
      "Epoch 209: 100%|██████████| 34/34 [00:00<00:00, 209.39batch/s, acc=1, loss=0.0272]     \n",
      "Epoch 210: 100%|██████████| 34/34 [00:00<00:00, 108.49batch/s, acc=1, loss=0.027]     \n",
      "Epoch 211: 100%|██████████| 34/34 [00:00<00:00, 138.21batch/s, acc=1, loss=0.0268]    \n",
      "Epoch 212: 100%|██████████| 34/34 [00:00<00:00, 154.55batch/s, acc=1, loss=0.0266]    \n",
      "Epoch 213: 100%|██████████| 34/34 [00:00<00:00, 103.60batch/s, acc=1, loss=0.0263]    \n",
      "Epoch 214: 100%|██████████| 34/34 [00:00<00:00, 86.35batch/s, acc=1, loss=0.0261]     \n",
      "Epoch 215: 100%|██████████| 34/34 [00:00<00:00, 67.30batch/s, acc=1, loss=0.0259]     \n",
      "Epoch 216: 100%|██████████| 34/34 [00:00<00:00, 83.53batch/s, acc=1, loss=0.0257]     \n",
      "Epoch 217: 100%|██████████| 34/34 [00:00<00:00, 114.01batch/s, acc=1, loss=0.0255]    \n",
      "Epoch 218: 100%|██████████| 34/34 [00:00<00:00, 134.54batch/s, acc=1, loss=0.0253]    \n",
      "Epoch 219: 100%|██████████| 34/34 [00:00<00:00, 140.36batch/s, acc=1, loss=0.0251]    \n",
      "Epoch 220: 100%|██████████| 34/34 [00:00<00:00, 199.85batch/s, acc=1, loss=0.0249]    \n",
      "Epoch 221: 100%|██████████| 34/34 [00:00<00:00, 75.49batch/s, acc=1, loss=0.0247]     \n",
      "Epoch 222: 100%|██████████| 34/34 [00:00<00:00, 160.14batch/s, acc=1, loss=0.0245]    \n",
      "Epoch 223: 100%|██████████| 34/34 [00:00<00:00, 196.20batch/s, acc=1, loss=0.0243]    \n",
      "Epoch 224: 100%|██████████| 34/34 [00:00<00:00, 124.32batch/s, acc=1, loss=0.0241]    \n",
      "Epoch 225: 100%|██████████| 34/34 [00:00<00:00, 108.76batch/s, acc=1, loss=0.0239]    \n",
      "Epoch 226: 100%|██████████| 34/34 [00:00<00:00, 115.09batch/s, acc=1, loss=0.0237]    \n",
      "Epoch 227: 100%|██████████| 34/34 [00:00<00:00, 120.37batch/s, acc=1, loss=0.0235]    \n",
      "Epoch 228: 100%|██████████| 34/34 [00:00<00:00, 72.70batch/s, acc=1, loss=0.0233]     \n",
      "Epoch 229: 100%|██████████| 34/34 [00:00<00:00, 139.10batch/s, acc=1, loss=0.0231]    \n",
      "Epoch 230: 100%|██████████| 34/34 [00:00<00:00, 240.35batch/s, acc=1, loss=0.023]     \n",
      "Epoch 231: 100%|██████████| 34/34 [00:00<00:00, 235.55batch/s, acc=1, loss=0.0228]    \n",
      "Epoch 232: 100%|██████████| 34/34 [00:00<00:00, 114.41batch/s, acc=1, loss=0.0226]    \n",
      "Epoch 233: 100%|██████████| 34/34 [00:00<00:00, 89.69batch/s, acc=1, loss=0.0224]     \n",
      "Epoch 234: 100%|██████████| 34/34 [00:00<00:00, 146.28batch/s, acc=1, loss=0.0222]    \n",
      "Epoch 235: 100%|██████████| 34/34 [00:00<00:00, 137.46batch/s, acc=1, loss=0.0221]    \n",
      "Epoch 236: 100%|██████████| 34/34 [00:00<00:00, 136.46batch/s, acc=1, loss=0.0219]    \n",
      "Epoch 237: 100%|██████████| 34/34 [00:00<00:00, 139.40batch/s, acc=1, loss=0.0217]    \n",
      "Epoch 238: 100%|██████████| 34/34 [00:00<00:00, 83.03batch/s, acc=1, loss=0.0216]     \n",
      "Epoch 239: 100%|██████████| 34/34 [00:00<00:00, 150.24batch/s, acc=1, loss=0.0214]    \n",
      "Epoch 240: 100%|██████████| 34/34 [00:00<00:00, 181.72batch/s, acc=1, loss=0.0212]    \n",
      "Epoch 241: 100%|██████████| 34/34 [00:00<00:00, 83.83batch/s, acc=1, loss=0.0211]     \n",
      "Epoch 242: 100%|██████████| 34/34 [00:00<00:00, 161.94batch/s, acc=1, loss=0.0209]    \n",
      "Epoch 243: 100%|██████████| 34/34 [00:00<00:00, 182.31batch/s, acc=1, loss=0.0207]    \n",
      "Epoch 244: 100%|██████████| 34/34 [00:00<00:00, 167.69batch/s, acc=1, loss=0.0206]    \n",
      "Epoch 245: 100%|██████████| 34/34 [00:00<00:00, 147.11batch/s, acc=1, loss=0.0204]    \n",
      "Epoch 246: 100%|██████████| 34/34 [00:00<00:00, 171.52batch/s, acc=1, loss=0.0202]    \n",
      "Epoch 247: 100%|██████████| 34/34 [00:00<00:00, 146.28batch/s, acc=1, loss=0.0201]    \n",
      "Epoch 248: 100%|██████████| 34/34 [00:00<00:00, 101.45batch/s, acc=1, loss=0.0199]    \n",
      "Epoch 249: 100%|██████████| 34/34 [00:00<00:00, 184.70batch/s, acc=1, loss=0.0198]    \n",
      "Epoch 250: 100%|██████████| 34/34 [00:00<00:00, 101.10batch/s, acc=1, loss=0.0196]    \n",
      "Epoch 251: 100%|██████████| 34/34 [00:00<00:00, 104.11batch/s, acc=1, loss=0.0195]    \n",
      "Epoch 252: 100%|██████████| 34/34 [00:00<00:00, 145.80batch/s, acc=1, loss=0.0193]    \n",
      "Epoch 253: 100%|██████████| 34/34 [00:00<00:00, 72.80batch/s, acc=1, loss=0.0192]     \n",
      "Epoch 254: 100%|██████████| 34/34 [00:00<00:00, 131.30batch/s, acc=1, loss=0.019]     \n",
      "Epoch 255: 100%|██████████| 34/34 [00:00<00:00, 163.94batch/s, acc=1, loss=0.0189]    \n",
      "Epoch 256: 100%|██████████| 34/34 [00:00<00:00, 132.86batch/s, acc=1, loss=0.0188]    \n",
      "Epoch 257: 100%|██████████| 34/34 [00:00<00:00, 167.98batch/s, acc=1, loss=0.0186]    \n",
      "Epoch 258: 100%|██████████| 34/34 [00:00<00:00, 77.52batch/s, acc=1, loss=0.0185]    \n",
      "Epoch 259: 100%|██████████| 34/34 [00:00<00:00, 99.63batch/s, acc=1, loss=0.0183]     \n",
      "Epoch 260: 100%|██████████| 34/34 [00:00<00:00, 125.95batch/s, acc=1, loss=0.0182]    \n",
      "Epoch 261: 100%|██████████| 34/34 [00:00<00:00, 72.87batch/s, acc=1, loss=0.0181]     \n",
      "Epoch 262: 100%|██████████| 34/34 [00:00<00:00, 179.42batch/s, acc=1, loss=0.0179]    \n",
      "Epoch 263: 100%|██████████| 34/34 [00:00<00:00, 110.47batch/s, acc=1, loss=0.0178]    \n",
      "Epoch 264: 100%|██████████| 34/34 [00:00<00:00, 47.46batch/s, acc=1, loss=0.0177]    \n",
      "Epoch 265: 100%|██████████| 34/34 [00:00<00:00, 97.04batch/s, acc=1, loss=0.0175]     \n",
      "Epoch 266: 100%|██████████| 34/34 [00:00<00:00, 98.54batch/s, acc=1, loss=0.0174]     \n",
      "Epoch 267: 100%|██████████| 34/34 [00:00<00:00, 72.55batch/s, acc=1, loss=0.0173]     \n",
      "Epoch 268: 100%|██████████| 34/34 [00:00<00:00, 121.95batch/s, acc=1, loss=0.0171]    \n",
      "Epoch 269: 100%|██████████| 34/34 [00:00<00:00, 140.07batch/s, acc=1, loss=0.017]     \n",
      "Epoch 270: 100%|██████████| 34/34 [00:00<00:00, 147.47batch/s, acc=1, loss=0.0169]    \n",
      "Epoch 271: 100%|██████████| 34/34 [00:00<00:00, 77.29batch/s, acc=1, loss=0.0168]     \n",
      "Epoch 272: 100%|██████████| 34/34 [00:00<00:00, 132.88batch/s, acc=1, loss=0.0166]    \n",
      "Epoch 273: 100%|██████████| 34/34 [00:00<00:00, 143.34batch/s, acc=1, loss=0.0165]    \n",
      "Epoch 274: 100%|██████████| 34/34 [00:00<00:00, 80.42batch/s, acc=1, loss=0.0164]     \n",
      "Epoch 275: 100%|██████████| 34/34 [00:00<00:00, 111.19batch/s, acc=1, loss=0.0163]    \n",
      "Epoch 276: 100%|██████████| 34/34 [00:00<00:00, 139.65batch/s, acc=1, loss=0.0162]    \n",
      "Epoch 277: 100%|██████████| 34/34 [00:00<00:00, 122.06batch/s, acc=1, loss=0.016]     \n",
      "Epoch 278: 100%|██████████| 34/34 [00:00<00:00, 58.82batch/s, acc=1, loss=0.0159]    \n",
      "Epoch 279: 100%|██████████| 34/34 [00:00<00:00, 93.53batch/s, acc=1, loss=0.0158]     \n",
      "Epoch 280: 100%|██████████| 34/34 [00:00<00:00, 125.88batch/s, acc=1, loss=0.0157]    \n",
      "Epoch 281: 100%|██████████| 34/34 [00:00<00:00, 59.41batch/s, acc=1, loss=0.0156]     \n",
      "Epoch 282: 100%|██████████| 34/34 [00:00<00:00, 108.94batch/s, acc=1, loss=0.0155]    \n",
      "Epoch 283: 100%|██████████| 34/34 [00:00<00:00, 106.51batch/s, acc=1, loss=0.0154]    \n",
      "Epoch 284: 100%|██████████| 34/34 [00:00<00:00, 74.26batch/s, acc=1, loss=0.0153]     \n",
      "Epoch 285: 100%|██████████| 34/34 [00:00<00:00, 78.97batch/s, acc=1, loss=0.0151]     \n",
      "Epoch 286: 100%|██████████| 34/34 [00:00<00:00, 108.69batch/s, acc=1, loss=0.015]     \n",
      "Epoch 287: 100%|██████████| 34/34 [00:00<00:00, 145.12batch/s, acc=1, loss=0.0149]    \n",
      "Epoch 288: 100%|██████████| 34/34 [00:00<00:00, 60.58batch/s, acc=1, loss=0.0148]     \n",
      "Epoch 289: 100%|██████████| 34/34 [00:00<00:00, 101.70batch/s, acc=1, loss=0.0147]    \n",
      "Epoch 290: 100%|██████████| 34/34 [00:00<00:00, 67.88batch/s, acc=1, loss=0.0146]     \n",
      "Epoch 291: 100%|██████████| 34/34 [00:00<00:00, 108.77batch/s, acc=1, loss=0.0145]    \n",
      "Epoch 292: 100%|██████████| 34/34 [00:00<00:00, 109.52batch/s, acc=1, loss=0.0144]    \n",
      "Epoch 293: 100%|██████████| 34/34 [00:00<00:00, 67.81batch/s, acc=1, loss=0.0143]     \n",
      "Epoch 294: 100%|██████████| 34/34 [00:00<00:00, 107.09batch/s, acc=1, loss=0.0142]    \n",
      "Epoch 295: 100%|██████████| 34/34 [00:00<00:00, 75.38batch/s, acc=1, loss=0.0141]     \n",
      "Epoch 296: 100%|██████████| 34/34 [00:00<00:00, 98.24batch/s, acc=1, loss=0.014]      \n",
      "Epoch 297: 100%|██████████| 34/34 [00:00<00:00, 97.80batch/s, acc=1, loss=0.0139]     \n",
      "Epoch 298: 100%|██████████| 34/34 [00:00<00:00, 57.89batch/s, acc=1, loss=0.0138]    \n",
      "Epoch 299: 100%|██████████| 34/34 [00:00<00:00, 105.77batch/s, acc=1, loss=0.0137]    \n",
      "Epoch 300: 100%|██████████| 34/34 [00:00<00:00, 142.43batch/s, acc=1, loss=0.0136]    \n",
      "Epoch 301: 100%|██████████| 34/34 [00:00<00:00, 77.18batch/s, acc=1, loss=0.0135]     \n",
      "Epoch 302: 100%|██████████| 34/34 [00:00<00:00, 84.51batch/s, acc=1, loss=0.0134]     \n",
      "Epoch 303: 100%|██████████| 34/34 [00:00<00:00, 119.58batch/s, acc=1, loss=0.0133]    \n",
      "Epoch 304: 100%|██████████| 34/34 [00:00<00:00, 68.98batch/s, acc=1, loss=0.0132]    \n",
      "Epoch 305: 100%|██████████| 34/34 [00:00<00:00, 75.82batch/s, acc=1, loss=0.0132]    \n",
      "Epoch 306: 100%|██████████| 34/34 [00:00<00:00, 66.34batch/s, acc=1, loss=0.0131]     \n",
      "Epoch 307: 100%|██████████| 34/34 [00:00<00:00, 118.52batch/s, acc=1, loss=0.013]     \n",
      "Epoch 308: 100%|██████████| 34/34 [00:00<00:00, 127.63batch/s, acc=1, loss=0.0129]    \n",
      "Epoch 309: 100%|██████████| 34/34 [00:00<00:00, 151.34batch/s, acc=1, loss=0.0128]    \n",
      "Epoch 310: 100%|██████████| 34/34 [00:00<00:00, 136.15batch/s, acc=1, loss=0.0127]    \n",
      "Epoch 311: 100%|██████████| 34/34 [00:00<00:00, 68.26batch/s, acc=1, loss=0.0126]     \n",
      "Epoch 312: 100%|██████████| 34/34 [00:00<00:00, 108.29batch/s, acc=1, loss=0.0125]    \n",
      "Epoch 313: 100%|██████████| 34/34 [00:00<00:00, 102.05batch/s, acc=1, loss=0.0124]    \n",
      "Epoch 314: 100%|██████████| 34/34 [00:00<00:00, 66.95batch/s, acc=1, loss=0.0124]     \n",
      "Epoch 315: 100%|██████████| 34/34 [00:00<00:00, 103.84batch/s, acc=1, loss=0.0123]    \n",
      "Epoch 316: 100%|██████████| 34/34 [00:00<00:00, 71.97batch/s, acc=1, loss=0.0122]     \n",
      "Epoch 317: 100%|██████████| 34/34 [00:00<00:00, 116.98batch/s, acc=1, loss=0.0121]    \n",
      "Epoch 318: 100%|██████████| 34/34 [00:00<00:00, 88.97batch/s, acc=1, loss=0.012]      \n",
      "Epoch 319: 100%|██████████| 34/34 [00:00<00:00, 109.16batch/s, acc=1, loss=0.012]     \n",
      "Epoch 320: 100%|██████████| 34/34 [00:00<00:00, 89.11batch/s, acc=1, loss=0.0119]     \n",
      "Epoch 321: 100%|██████████| 34/34 [00:00<00:00, 54.74batch/s, acc=1, loss=0.0118]    \n",
      "Epoch 322: 100%|██████████| 34/34 [00:00<00:00, 128.54batch/s, acc=1, loss=0.0117]    \n",
      "Epoch 323: 100%|██████████| 34/34 [00:00<00:00, 85.55batch/s, acc=1, loss=0.0116]     \n",
      "Epoch 324: 100%|██████████| 34/34 [00:00<00:00, 87.69batch/s, acc=1, loss=0.0116]     \n",
      "Epoch 325: 100%|██████████| 34/34 [00:00<00:00, 108.27batch/s, acc=1, loss=0.0115]    \n",
      "Epoch 326: 100%|██████████| 34/34 [00:00<00:00, 152.07batch/s, acc=1, loss=0.0114]    \n",
      "Epoch 327: 100%|██████████| 34/34 [00:00<00:00, 82.40batch/s, acc=1, loss=0.0113]     \n",
      "Epoch 328: 100%|██████████| 34/34 [00:00<00:00, 72.73batch/s, acc=1, loss=0.0113]     \n",
      "Epoch 329: 100%|██████████| 34/34 [00:00<00:00, 74.58batch/s, acc=1, loss=0.0112]     \n",
      "Epoch 330: 100%|██████████| 34/34 [00:00<00:00, 94.35batch/s, acc=1, loss=0.0111]     \n",
      "Epoch 331: 100%|██████████| 34/34 [00:00<00:00, 150.42batch/s, acc=1, loss=0.011]     \n",
      "Epoch 332: 100%|██████████| 34/34 [00:00<00:00, 67.38batch/s, acc=1, loss=0.011]     \n",
      "Epoch 333: 100%|██████████| 34/34 [00:00<00:00, 83.18batch/s, acc=1, loss=0.0109]     \n",
      "Epoch 334: 100%|██████████| 34/34 [00:00<00:00, 71.82batch/s, acc=1, loss=0.0108]    \n",
      "Epoch 335: 100%|██████████| 34/34 [00:00<00:00, 69.33batch/s, acc=1, loss=0.0107]     \n",
      "Epoch 336: 100%|██████████| 34/34 [00:00<00:00, 80.71batch/s, acc=1, loss=0.0107]     \n",
      "Epoch 337: 100%|██████████| 34/34 [00:00<00:00, 106.37batch/s, acc=1, loss=0.0106]    \n",
      "Epoch 338: 100%|██████████| 34/34 [00:00<00:00, 108.64batch/s, acc=1, loss=0.0105]    \n",
      "Epoch 339: 100%|██████████| 34/34 [00:00<00:00, 90.25batch/s, acc=1, loss=0.0105]     \n",
      "Epoch 340: 100%|██████████| 34/34 [00:00<00:00, 153.13batch/s, acc=1, loss=0.0104]    \n",
      "Epoch 341: 100%|██████████| 34/34 [00:00<00:00, 168.86batch/s, acc=1, loss=0.0103]    \n",
      "Epoch 342: 100%|██████████| 34/34 [00:00<00:00, 136.83batch/s, acc=1, loss=0.0103]    \n",
      "Epoch 343: 100%|██████████| 34/34 [00:00<00:00, 87.09batch/s, acc=1, loss=0.0102]     \n",
      "Epoch 344: 100%|██████████| 34/34 [00:00<00:00, 99.17batch/s, acc=1, loss=0.0101]     \n",
      "Epoch 345: 100%|██████████| 34/34 [00:00<00:00, 132.45batch/s, acc=1, loss=0.0101]    \n",
      "Epoch 346: 100%|██████████| 34/34 [00:00<00:00, 104.89batch/s, acc=1, loss=0.01]      \n",
      "Epoch 347: 100%|██████████| 34/34 [00:00<00:00, 127.00batch/s, acc=1, loss=0.00995]   \n",
      "Epoch 348: 100%|██████████| 34/34 [00:00<00:00, 72.23batch/s, acc=1, loss=0.00988]   \n",
      "Epoch 349: 100%|██████████| 34/34 [00:00<00:00, 75.21batch/s, acc=1, loss=0.00982]    \n",
      "Epoch 350: 100%|██████████| 34/34 [00:00<00:00, 62.11batch/s, acc=1, loss=0.00976]    \n",
      "Epoch 351: 100%|██████████| 34/34 [00:00<00:00, 121.84batch/s, acc=1, loss=0.0097]    \n",
      "Epoch 352: 100%|██████████| 34/34 [00:00<00:00, 106.44batch/s, acc=1, loss=0.00963]   \n",
      "Epoch 353: 100%|██████████| 34/34 [00:00<00:00, 94.33batch/s, acc=1, loss=0.00957]    \n",
      "Epoch 354: 100%|██████████| 34/34 [00:00<00:00, 147.86batch/s, acc=1, loss=0.00951]   \n",
      "Epoch 355: 100%|██████████| 34/34 [00:00<00:00, 68.87batch/s, acc=1, loss=0.00945]    \n",
      "Epoch 356: 100%|██████████| 34/34 [00:00<00:00, 117.61batch/s, acc=1, loss=0.00939]   \n",
      "Epoch 357: 100%|██████████| 34/34 [00:00<00:00, 149.48batch/s, acc=1, loss=0.00933]   \n",
      "Epoch 358: 100%|██████████| 34/34 [00:00<00:00, 119.78batch/s, acc=1, loss=0.00928]   \n",
      "Epoch 359: 100%|██████████| 34/34 [00:00<00:00, 127.86batch/s, acc=1, loss=0.00922]   \n",
      "Epoch 360: 100%|██████████| 34/34 [00:00<00:00, 54.41batch/s, acc=1, loss=0.00916]   \n",
      "Epoch 361: 100%|██████████| 34/34 [00:00<00:00, 132.31batch/s, acc=1, loss=0.0091]    \n",
      "Epoch 362: 100%|██████████| 34/34 [00:00<00:00, 76.79batch/s, acc=1, loss=0.00905]    \n",
      "Epoch 363: 100%|██████████| 34/34 [00:00<00:00, 57.28batch/s, acc=1, loss=0.00899]   \n",
      "Epoch 364: 100%|██████████| 34/34 [00:00<00:00, 90.00batch/s, acc=1, loss=0.00893]    \n",
      "Epoch 365: 100%|██████████| 34/34 [00:00<00:00, 69.42batch/s, acc=1, loss=0.00888]    \n",
      "Epoch 366: 100%|██████████| 34/34 [00:00<00:00, 76.99batch/s, acc=1, loss=0.00882]   \n",
      "Epoch 367: 100%|██████████| 34/34 [00:00<00:00, 103.20batch/s, acc=1, loss=0.00877]   \n",
      "Epoch 368: 100%|██████████| 34/34 [00:00<00:00, 157.41batch/s, acc=1, loss=0.00872]   \n",
      "Epoch 369: 100%|██████████| 34/34 [00:00<00:00, 91.41batch/s, acc=1, loss=0.00866]    \n",
      "Epoch 370: 100%|██████████| 34/34 [00:00<00:00, 131.29batch/s, acc=1, loss=0.00861]   \n",
      "Epoch 371: 100%|██████████| 34/34 [00:00<00:00, 174.27batch/s, acc=1, loss=0.00856]   \n",
      "Epoch 372: 100%|██████████| 34/34 [00:00<00:00, 95.82batch/s, acc=1, loss=0.0085]     \n",
      "Epoch 373: 100%|██████████| 34/34 [00:00<00:00, 96.69batch/s, acc=1, loss=0.00845]    \n",
      "Epoch 374: 100%|██████████| 34/34 [00:00<00:00, 122.90batch/s, acc=1, loss=0.0084]    \n",
      "Epoch 375: 100%|██████████| 34/34 [00:00<00:00, 58.63batch/s, acc=1, loss=0.00835]   \n",
      "Epoch 376: 100%|██████████| 34/34 [00:00<00:00, 109.86batch/s, acc=1, loss=0.0083]    \n",
      "Epoch 377: 100%|██████████| 34/34 [00:00<00:00, 113.03batch/s, acc=1, loss=0.00825]   \n",
      "Epoch 378: 100%|██████████| 34/34 [00:00<00:00, 109.11batch/s, acc=1, loss=0.0082]    \n",
      "Epoch 379: 100%|██████████| 34/34 [00:00<00:00, 75.95batch/s, acc=1, loss=0.00815]    \n",
      "Epoch 380: 100%|██████████| 34/34 [00:00<00:00, 118.44batch/s, acc=1, loss=0.0081]    \n",
      "Epoch 381: 100%|██████████| 34/34 [00:00<00:00, 143.77batch/s, acc=1, loss=0.00805]   \n",
      "Epoch 382: 100%|██████████| 34/34 [00:00<00:00, 111.87batch/s, acc=1, loss=0.008]     \n",
      "Epoch 383: 100%|██████████| 34/34 [00:00<00:00, 156.86batch/s, acc=1, loss=0.00796]   \n",
      "Epoch 384: 100%|██████████| 34/34 [00:00<00:00, 74.42batch/s, acc=1, loss=0.00791]   \n",
      "Epoch 385: 100%|██████████| 34/34 [00:00<00:00, 131.10batch/s, acc=1, loss=0.00786]   \n",
      "Epoch 386: 100%|██████████| 34/34 [00:00<00:00, 138.99batch/s, acc=1, loss=0.00781]   \n",
      "Epoch 387: 100%|██████████| 34/34 [00:00<00:00, 91.58batch/s, acc=1, loss=0.00777]   \n",
      "Epoch 388: 100%|██████████| 34/34 [00:00<00:00, 96.54batch/s, acc=1, loss=0.00772]   \n",
      "Epoch 389: 100%|██████████| 34/34 [00:00<00:00, 70.87batch/s, acc=1, loss=0.00767]    \n",
      "Epoch 390: 100%|██████████| 34/34 [00:00<00:00, 112.51batch/s, acc=1, loss=0.00763]   \n",
      "Epoch 391: 100%|██████████| 34/34 [00:00<00:00, 155.68batch/s, acc=1, loss=0.00758]   \n",
      "Epoch 392: 100%|██████████| 34/34 [00:00<00:00, 103.67batch/s, acc=1, loss=0.00754]   \n",
      "Epoch 393: 100%|██████████| 34/34 [00:00<00:00, 94.48batch/s, acc=1, loss=0.00749]   \n",
      "Epoch 394: 100%|██████████| 34/34 [00:00<00:00, 96.29batch/s, acc=1, loss=0.00745]    \n",
      "Epoch 395: 100%|██████████| 34/34 [00:00<00:00, 155.74batch/s, acc=1, loss=0.00741]   \n",
      "Epoch 396: 100%|██████████| 34/34 [00:00<00:00, 72.64batch/s, acc=1, loss=0.00736]    \n",
      "Epoch 397: 100%|██████████| 34/34 [00:00<00:00, 133.55batch/s, acc=1, loss=0.00732]   \n",
      "Epoch 398: 100%|██████████| 34/34 [00:00<00:00, 149.93batch/s, acc=1, loss=0.00728]   \n",
      "Epoch 399: 100%|██████████| 34/34 [00:00<00:00, 81.45batch/s, acc=1, loss=0.00724]    \n",
      "Epoch 400: 100%|██████████| 34/34 [00:00<00:00, 117.39batch/s, acc=1, loss=0.00719]   \n",
      "Epoch 401: 100%|██████████| 34/34 [00:00<00:00, 89.71batch/s, acc=1, loss=0.00715]    \n",
      "Epoch 402: 100%|██████████| 34/34 [00:00<00:00, 124.94batch/s, acc=1, loss=0.00711]   \n",
      "Epoch 403: 100%|██████████| 34/34 [00:00<00:00, 71.02batch/s, acc=1, loss=0.00707]    \n",
      "Epoch 404: 100%|██████████| 34/34 [00:00<00:00, 104.46batch/s, acc=1, loss=0.00703]   \n",
      "Epoch 405: 100%|██████████| 34/34 [00:00<00:00, 116.37batch/s, acc=1, loss=0.00699]   \n",
      "Epoch 406: 100%|██████████| 34/34 [00:00<00:00, 159.56batch/s, acc=1, loss=0.00695]   \n",
      "Epoch 407: 100%|██████████| 34/34 [00:00<00:00, 77.50batch/s, acc=1, loss=0.00691]    \n",
      "Epoch 408: 100%|██████████| 34/34 [00:00<00:00, 179.44batch/s, acc=1, loss=0.00687]   \n",
      "Epoch 409: 100%|██████████| 34/34 [00:00<00:00, 92.04batch/s, acc=1, loss=0.00683]    \n",
      "Epoch 410: 100%|██████████| 34/34 [00:00<00:00, 107.66batch/s, acc=1, loss=0.00679]   \n",
      "Epoch 411: 100%|██████████| 34/34 [00:00<00:00, 86.57batch/s, acc=1, loss=0.00675]    \n",
      "Epoch 412: 100%|██████████| 34/34 [00:00<00:00, 134.26batch/s, acc=1, loss=0.00671]   \n",
      "Epoch 413: 100%|██████████| 34/34 [00:00<00:00, 191.65batch/s, acc=1, loss=0.00667]   \n",
      "Epoch 414: 100%|██████████| 34/34 [00:00<00:00, 192.39batch/s, acc=1, loss=0.00663]   \n",
      "Epoch 415: 100%|██████████| 34/34 [00:00<00:00, 154.27batch/s, acc=1, loss=0.0066]    \n",
      "Epoch 416: 100%|██████████| 34/34 [00:00<00:00, 70.02batch/s, acc=1, loss=0.00656]    \n",
      "Epoch 417: 100%|██████████| 34/34 [00:00<00:00, 119.79batch/s, acc=1, loss=0.00652]   \n",
      "Epoch 418: 100%|██████████| 34/34 [00:00<00:00, 55.56batch/s, acc=1, loss=0.00649]   \n",
      "Epoch 419: 100%|██████████| 34/34 [00:00<00:00, 138.04batch/s, acc=1, loss=0.00645]   \n",
      "Epoch 420: 100%|██████████| 34/34 [00:00<00:00, 85.85batch/s, acc=1, loss=0.00641]    \n",
      "Epoch 421: 100%|██████████| 34/34 [00:00<00:00, 144.05batch/s, acc=1, loss=0.00638]   \n",
      "Epoch 422: 100%|██████████| 34/34 [00:00<00:00, 200.41batch/s, acc=1, loss=0.00634]   \n",
      "Epoch 423: 100%|██████████| 34/34 [00:00<00:00, 154.29batch/s, acc=1, loss=0.00631]   \n",
      "Epoch 424: 100%|██████████| 34/34 [00:00<00:00, 140.31batch/s, acc=1, loss=0.00627]   \n",
      "Epoch 425: 100%|██████████| 34/34 [00:00<00:00, 111.36batch/s, acc=1, loss=0.00623]   \n",
      "Epoch 426: 100%|██████████| 34/34 [00:00<00:00, 87.26batch/s, acc=1, loss=0.0062]     \n",
      "Epoch 427: 100%|██████████| 34/34 [00:00<00:00, 98.17batch/s, acc=1, loss=0.00617]    \n",
      "Epoch 428: 100%|██████████| 34/34 [00:00<00:00, 109.70batch/s, acc=1, loss=0.00613]   \n",
      "Epoch 429: 100%|██████████| 34/34 [00:00<00:00, 206.75batch/s, acc=1, loss=0.0061]    \n",
      "Epoch 430: 100%|██████████| 34/34 [00:00<00:00, 96.43batch/s, acc=1, loss=0.00606]    \n",
      "Epoch 431: 100%|██████████| 34/34 [00:00<00:00, 126.21batch/s, acc=1, loss=0.00603]   \n",
      "Epoch 432: 100%|██████████| 34/34 [00:00<00:00, 181.64batch/s, acc=1, loss=0.006]     \n",
      "Epoch 433: 100%|██████████| 34/34 [00:00<00:00, 80.24batch/s, acc=1, loss=0.00596]    \n",
      "Epoch 434: 100%|██████████| 34/34 [00:00<00:00, 181.73batch/s, acc=1, loss=0.00593]   \n",
      "Epoch 435: 100%|██████████| 34/34 [00:00<00:00, 154.11batch/s, acc=1, loss=0.0059]    \n",
      "Epoch 436: 100%|██████████| 34/34 [00:00<00:00, 179.88batch/s, acc=1, loss=0.00587]   \n",
      "Epoch 437: 100%|██████████| 34/34 [00:00<00:00, 130.48batch/s, acc=1, loss=0.00583]   \n",
      "Epoch 438: 100%|██████████| 34/34 [00:00<00:00, 95.36batch/s, acc=1, loss=0.0058]     \n",
      "Epoch 439: 100%|██████████| 34/34 [00:00<00:00, 145.91batch/s, acc=1, loss=0.00577]   \n",
      "Epoch 440: 100%|██████████| 34/34 [00:00<00:00, 176.77batch/s, acc=1, loss=0.00574]   \n",
      "Epoch 441: 100%|██████████| 34/34 [00:00<00:00, 119.88batch/s, acc=1, loss=0.00571]   \n",
      "Epoch 442: 100%|██████████| 34/34 [00:00<00:00, 81.59batch/s, acc=1, loss=0.00568]    \n",
      "Epoch 443: 100%|██████████| 34/34 [00:00<00:00, 135.12batch/s, acc=1, loss=0.00565]   \n",
      "Epoch 444: 100%|██████████| 34/34 [00:00<00:00, 106.95batch/s, acc=1, loss=0.00561]   \n",
      "Epoch 445: 100%|██████████| 34/34 [00:00<00:00, 138.65batch/s, acc=1, loss=0.00558]   \n",
      "Epoch 446: 100%|██████████| 34/34 [00:00<00:00, 99.42batch/s, acc=1, loss=0.00555]    \n",
      "Epoch 447: 100%|██████████| 34/34 [00:00<00:00, 163.67batch/s, acc=1, loss=0.00552]   \n",
      "Epoch 448: 100%|██████████| 34/34 [00:00<00:00, 161.30batch/s, acc=1, loss=0.00549]   \n",
      "Epoch 449: 100%|██████████| 34/34 [00:00<00:00, 164.29batch/s, acc=1, loss=0.00547]   \n",
      "Epoch 450: 100%|██████████| 34/34 [00:00<00:00, 124.73batch/s, acc=1, loss=0.00544]   \n",
      "Epoch 451: 100%|██████████| 34/34 [00:00<00:00, 145.38batch/s, acc=1, loss=0.00541]   \n",
      "Epoch 452: 100%|██████████| 34/34 [00:00<00:00, 97.87batch/s, acc=1, loss=0.00538]    \n",
      "Epoch 453: 100%|██████████| 34/34 [00:00<00:00, 108.76batch/s, acc=1, loss=0.00535]   \n",
      "Epoch 454: 100%|██████████| 34/34 [00:00<00:00, 136.59batch/s, acc=1, loss=0.00532]   \n",
      "Epoch 455: 100%|██████████| 34/34 [00:00<00:00, 105.69batch/s, acc=1, loss=0.00529]   \n",
      "Epoch 456: 100%|██████████| 34/34 [00:00<00:00, 141.06batch/s, acc=1, loss=0.00526]   \n",
      "Epoch 457: 100%|██████████| 34/34 [00:00<00:00, 142.72batch/s, acc=1, loss=0.00524]   \n",
      "Epoch 458: 100%|██████████| 34/34 [00:00<00:00, 213.77batch/s, acc=1, loss=0.00521]   \n",
      "Epoch 459: 100%|██████████| 34/34 [00:00<00:00, 193.77batch/s, acc=1, loss=0.00518]   \n",
      "Epoch 460: 100%|██████████| 34/34 [00:00<00:00, 115.71batch/s, acc=1, loss=0.00515]   \n",
      "Epoch 461: 100%|██████████| 34/34 [00:00<00:00, 141.46batch/s, acc=1, loss=0.00513]   \n",
      "Epoch 462: 100%|██████████| 34/34 [00:00<00:00, 83.94batch/s, acc=1, loss=0.0051]     \n",
      "Epoch 463: 100%|██████████| 34/34 [00:00<00:00, 164.07batch/s, acc=1, loss=0.00507]   \n",
      "Epoch 464: 100%|██████████| 34/34 [00:00<00:00, 97.67batch/s, acc=1, loss=0.00505]   \n",
      "Epoch 465: 100%|██████████| 34/34 [00:00<00:00, 89.51batch/s, acc=1, loss=0.00502]    \n",
      "Epoch 466: 100%|██████████| 34/34 [00:00<00:00, 83.19batch/s, acc=1, loss=0.00499]   \n",
      "Epoch 467: 100%|██████████| 34/34 [00:00<00:00, 71.62batch/s, acc=1, loss=0.00497]    \n",
      "Epoch 468: 100%|██████████| 34/34 [00:00<00:00, 94.82batch/s, acc=1, loss=0.00494]    \n",
      "Epoch 469: 100%|██████████| 34/34 [00:00<00:00, 54.14batch/s, acc=1, loss=0.00492]    \n",
      "Epoch 470: 100%|██████████| 34/34 [00:00<00:00, 71.71batch/s, acc=1, loss=0.00489]   \n",
      "Epoch 471: 100%|██████████| 34/34 [00:00<00:00, 77.05batch/s, acc=1, loss=0.00487]   \n",
      "Epoch 472: 100%|██████████| 34/34 [00:00<00:00, 45.91batch/s, acc=1, loss=0.00484]   \n",
      "Epoch 473: 100%|██████████| 34/34 [00:00<00:00, 72.54batch/s, acc=1, loss=0.00481]   \n",
      "Epoch 474: 100%|██████████| 34/34 [00:00<00:00, 82.87batch/s, acc=1, loss=0.00479]   \n",
      "Epoch 475: 100%|██████████| 34/34 [00:00<00:00, 63.73batch/s, acc=1, loss=0.00477]   \n",
      "Epoch 476: 100%|██████████| 34/34 [00:00<00:00, 115.46batch/s, acc=1, loss=0.00474]   \n",
      "Epoch 477: 100%|██████████| 34/34 [00:00<00:00, 160.62batch/s, acc=1, loss=0.00472]   \n",
      "Epoch 478: 100%|██████████| 34/34 [00:00<00:00, 80.23batch/s, acc=1, loss=0.00469]    \n",
      "Epoch 479: 100%|██████████| 34/34 [00:00<00:00, 101.86batch/s, acc=1, loss=0.00467]   \n",
      "Epoch 480: 100%|██████████| 34/34 [00:00<00:00, 68.14batch/s, acc=1, loss=0.00464]    \n",
      "Epoch 481: 100%|██████████| 34/34 [00:00<00:00, 104.36batch/s, acc=1, loss=0.00462]   \n",
      "Epoch 482: 100%|██████████| 34/34 [00:00<00:00, 72.26batch/s, acc=1, loss=0.0046]     \n",
      "Epoch 483: 100%|██████████| 34/34 [00:00<00:00, 114.57batch/s, acc=1, loss=0.00457]   \n",
      "Epoch 484: 100%|██████████| 34/34 [00:00<00:00, 132.86batch/s, acc=1, loss=0.00455]   \n",
      "Epoch 485: 100%|██████████| 34/34 [00:00<00:00, 114.32batch/s, acc=1, loss=0.00453]   \n",
      "Epoch 486: 100%|██████████| 34/34 [00:00<00:00, 165.41batch/s, acc=1, loss=0.0045]    \n",
      "Epoch 487: 100%|██████████| 34/34 [00:00<00:00, 68.70batch/s, acc=1, loss=0.00448]    \n",
      "Epoch 488: 100%|██████████| 34/34 [00:00<00:00, 141.46batch/s, acc=1, loss=0.00446]   \n",
      "Epoch 489: 100%|██████████| 34/34 [00:00<00:00, 123.40batch/s, acc=1, loss=0.00444]   \n",
      "Epoch 490: 100%|██████████| 34/34 [00:00<00:00, 172.49batch/s, acc=1, loss=0.00441]   \n",
      "Epoch 491: 100%|██████████| 34/34 [00:00<00:00, 102.84batch/s, acc=1, loss=0.00439]   \n",
      "Epoch 492: 100%|██████████| 34/34 [00:00<00:00, 80.66batch/s, acc=1, loss=0.00437]    \n",
      "Epoch 493: 100%|██████████| 34/34 [00:00<00:00, 146.50batch/s, acc=1, loss=0.00435]   \n",
      "Epoch 494: 100%|██████████| 34/34 [00:00<00:00, 64.16batch/s, acc=1, loss=0.00433]    \n",
      "Epoch 495: 100%|██████████| 34/34 [00:00<00:00, 96.51batch/s, acc=1, loss=0.00431]    \n",
      "Epoch 496: 100%|██████████| 34/34 [00:00<00:00, 170.31batch/s, acc=1, loss=0.00428]   \n",
      "Epoch 497: 100%|██████████| 34/34 [00:00<00:00, 88.84batch/s, acc=1, loss=0.00426]    \n",
      "Epoch 498: 100%|██████████| 34/34 [00:00<00:00, 147.77batch/s, acc=1, loss=0.00424]   \n",
      "Epoch 499: 100%|██████████| 34/34 [00:00<00:00, 80.98batch/s, acc=1, loss=0.00422]    \n"
     ]
    }
   ],
   "source": [
    "clf = sg_detection.LinearClassifier(input_size=512)\n",
    "acc = train_classifier(clf, feats, stacked_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def classifier_performance(clf, X, labels):\n",
    "    y = torch.tensor(labels).float().squeeze()\n",
    "    y_pred = clf(X)\n",
    "\n",
    "\n",
    "    acc = (y_pred.round() == y).float().mean()\n",
    "    print(\"Accuracy: \", acc)\n",
    "    y_pred_np = y_pred.detach().numpy()\n",
    "    c_m = confusion_matrix(labels, y_pred_np.round())\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(c_m)\n",
    "    return acc, c_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  tensor(0.7764)\n",
      "Confusion Matrix: \n",
      "[[1170   16    0]\n",
      " [   5  162    0]\n",
      " [   0    1    0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7764),\n",
       " array([[1170,   16,    0],\n",
       "        [   5,  162,    0],\n",
       "        [   0,    1,    0]]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_performance(clf, feats, stacked_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 34/34 [00:00<00:00, 162.65batch/s, acc=0.926, loss=0.8]  \n",
      "Epoch 1: 100%|██████████| 34/34 [00:00<00:00, 92.14batch/s, acc=0.963, loss=0.61]  \n",
      "Epoch 2: 100%|██████████| 34/34 [00:00<00:00, 180.30batch/s, acc=0.963, loss=0.496]\n",
      "Epoch 3: 100%|██████████| 34/34 [00:00<00:00, 224.10batch/s, acc=0.963, loss=0.411]\n",
      "Epoch 4: 100%|██████████| 34/34 [00:00<00:00, 239.95batch/s, acc=0.963, loss=0.345]\n",
      "Epoch 5: 100%|██████████| 34/34 [00:00<00:00, 235.24batch/s, acc=0.963, loss=0.293]\n",
      "Epoch 6: 100%|██████████| 34/34 [00:00<00:00, 246.98batch/s, acc=0.963, loss=0.254]\n",
      "Epoch 7: 100%|██████████| 34/34 [00:00<00:00, 232.41batch/s, acc=0.963, loss=0.223]\n",
      "Epoch 8: 100%|██████████| 34/34 [00:00<00:00, 241.11batch/s, acc=0.963, loss=0.199]\n",
      "Epoch 9: 100%|██████████| 34/34 [00:00<00:00, 232.45batch/s, acc=0.963, loss=0.178]\n",
      "Epoch 10: 100%|██████████| 34/34 [00:00<00:00, 235.22batch/s, acc=0.963, loss=0.16] \n",
      "Epoch 11: 100%|██████████| 34/34 [00:00<00:00, 227.67batch/s, acc=0.963, loss=0.146]\n",
      "Epoch 12: 100%|██████████| 34/34 [00:00<00:00, 234.36batch/s, acc=1, loss=0.133]    \n",
      "Epoch 13: 100%|██████████| 34/34 [00:00<00:00, 230.47batch/s, acc=1, loss=0.122]    \n",
      "Epoch 14: 100%|██████████| 34/34 [00:00<00:00, 185.32batch/s, acc=1, loss=0.114]    \n",
      "Epoch 15: 100%|██████████| 34/34 [00:00<00:00, 87.17batch/s, acc=1, loss=0.105]     \n",
      "Epoch 16: 100%|██████████| 34/34 [00:00<00:00, 126.61batch/s, acc=1, loss=0.0973]   \n",
      "Epoch 17: 100%|██████████| 34/34 [00:00<00:00, 125.92batch/s, acc=1, loss=0.0908]    \n",
      "Epoch 18: 100%|██████████| 34/34 [00:00<00:00, 90.07batch/s, acc=1, loss=0.0849]     \n",
      "Epoch 19: 100%|██████████| 34/34 [00:00<00:00, 233.73batch/s, acc=1, loss=0.0786]    \n",
      "Epoch 20: 100%|██████████| 34/34 [00:00<00:00, 219.52batch/s, acc=1, loss=0.0735]    \n",
      "Epoch 21: 100%|██████████| 34/34 [00:00<00:00, 136.05batch/s, acc=1, loss=0.0692]    \n",
      "Epoch 22: 100%|██████████| 34/34 [00:00<00:00, 189.37batch/s, acc=1, loss=0.065]     \n",
      "Epoch 23: 100%|██████████| 34/34 [00:00<00:00, 207.19batch/s, acc=1, loss=0.0609]    \n",
      "Epoch 24: 100%|██████████| 34/34 [00:00<00:00, 74.58batch/s, acc=1, loss=0.0572]    \n",
      "Epoch 25: 100%|██████████| 34/34 [00:00<00:00, 222.55batch/s, acc=1, loss=0.0534]   \n",
      "Epoch 26: 100%|██████████| 34/34 [00:00<00:00, 177.57batch/s, acc=1, loss=0.0507]    \n",
      "Epoch 27: 100%|██████████| 34/34 [00:00<00:00, 186.91batch/s, acc=1, loss=0.0473]    \n",
      "Epoch 28: 100%|██████████| 34/34 [00:00<00:00, 150.39batch/s, acc=1, loss=0.0448]    \n",
      "Epoch 29: 100%|██████████| 34/34 [00:00<00:00, 224.36batch/s, acc=1, loss=0.0421]    \n",
      "Epoch 30: 100%|██████████| 34/34 [00:01<00:00, 33.88batch/s, acc=1, loss=0.0399]    \n",
      "Epoch 31: 100%|██████████| 34/34 [00:00<00:00, 184.43batch/s, acc=1, loss=0.0375]    \n",
      "Epoch 32: 100%|██████████| 34/34 [00:00<00:00, 225.16batch/s, acc=1, loss=0.0359]     \n",
      "Epoch 33: 100%|██████████| 34/34 [00:00<00:00, 188.96batch/s, acc=1, loss=0.0335]    \n",
      "Epoch 34: 100%|██████████| 34/34 [00:00<00:00, 171.51batch/s, acc=1, loss=0.0322]    \n",
      "Epoch 35: 100%|██████████| 34/34 [00:00<00:00, 196.18batch/s, acc=1, loss=0.0299]    \n",
      "Epoch 36: 100%|██████████| 34/34 [00:00<00:00, 160.59batch/s, acc=1, loss=0.0289]    \n",
      "Epoch 37: 100%|██████████| 34/34 [00:00<00:00, 200.94batch/s, acc=1, loss=0.0267]    \n",
      "Epoch 38: 100%|██████████| 34/34 [00:00<00:00, 97.53batch/s, acc=1, loss=0.026]      \n",
      "Epoch 39: 100%|██████████| 34/34 [00:00<00:00, 199.95batch/s, acc=1, loss=0.0241]    \n",
      "Epoch 40: 100%|██████████| 34/34 [00:00<00:00, 210.85batch/s, acc=1, loss=0.0234]    \n",
      "Epoch 41: 100%|██████████| 34/34 [00:00<00:00, 218.25batch/s, acc=1, loss=0.0217]    \n",
      "Epoch 42: 100%|██████████| 34/34 [00:00<00:00, 213.35batch/s, acc=1, loss=0.021]     \n",
      "Epoch 43: 100%|██████████| 34/34 [00:00<00:00, 209.72batch/s, acc=1, loss=0.0198]    \n",
      "Epoch 44: 100%|██████████| 34/34 [00:00<00:00, 217.96batch/s, acc=1, loss=0.0188]    \n",
      "Epoch 45: 100%|██████████| 34/34 [00:00<00:00, 215.83batch/s, acc=1, loss=0.0181]    \n",
      "Epoch 46: 100%|██████████| 34/34 [00:00<00:00, 210.32batch/s, acc=1, loss=0.0169]    \n",
      "Epoch 47: 100%|██████████| 34/34 [00:00<00:00, 173.18batch/s, acc=1, loss=0.0164]    \n",
      "Epoch 48: 100%|██████████| 34/34 [00:00<00:00, 89.16batch/s, acc=1, loss=0.0155]    \n",
      "Epoch 49: 100%|██████████| 34/34 [00:00<00:00, 194.40batch/s, acc=1, loss=0.0146]    \n",
      "Epoch 50: 100%|██████████| 34/34 [00:00<00:00, 195.56batch/s, acc=1, loss=0.0141]    \n",
      "Epoch 51: 100%|██████████| 34/34 [00:00<00:00, 185.84batch/s, acc=1, loss=0.0134]    \n",
      "Epoch 52: 100%|██████████| 34/34 [00:00<00:00, 189.28batch/s, acc=1, loss=0.0127]    \n",
      "Epoch 53: 100%|██████████| 34/34 [00:00<00:00, 195.89batch/s, acc=1, loss=0.0123]    \n",
      "Epoch 54: 100%|██████████| 34/34 [00:00<00:00, 212.95batch/s, acc=1, loss=0.0115]    \n",
      "Epoch 55: 100%|██████████| 34/34 [00:00<00:00, 200.63batch/s, acc=1, loss=0.011]     \n",
      "Epoch 56: 100%|██████████| 34/34 [00:00<00:00, 97.98batch/s, acc=1, loss=0.0105]     \n",
      "Epoch 57: 100%|██████████| 34/34 [00:00<00:00, 170.26batch/s, acc=1, loss=0.01]      \n",
      "Epoch 58: 100%|██████████| 34/34 [00:00<00:00, 218.38batch/s, acc=1, loss=0.00958]   \n",
      "Epoch 59: 100%|██████████| 34/34 [00:00<00:00, 209.08batch/s, acc=1, loss=0.00918]   \n",
      "Epoch 60: 100%|██████████| 34/34 [00:00<00:00, 226.98batch/s, acc=1, loss=0.0087]    \n",
      "Epoch 61: 100%|██████████| 34/34 [00:00<00:00, 139.66batch/s, acc=1, loss=0.0082]    \n",
      "Epoch 62: 100%|██████████| 34/34 [00:00<00:00, 159.62batch/s, acc=1, loss=0.00792]   \n",
      "Epoch 63: 100%|██████████| 34/34 [00:00<00:00, 204.69batch/s, acc=1, loss=0.00753]   \n",
      "Epoch 64: 100%|██████████| 34/34 [00:00<00:00, 84.93batch/s, acc=1, loss=0.00716]    \n",
      "Epoch 65: 100%|██████████| 34/34 [00:00<00:00, 103.06batch/s, acc=1, loss=0.00691]   \n",
      "Epoch 66: 100%|██████████| 34/34 [00:00<00:00, 133.37batch/s, acc=1, loss=0.00644]   \n",
      "Epoch 67: 100%|██████████| 34/34 [00:00<00:00, 214.13batch/s, acc=1, loss=0.00627]   \n",
      "Epoch 68: 100%|██████████| 34/34 [00:00<00:00, 215.15batch/s, acc=1, loss=0.00583]   \n",
      "Epoch 69: 100%|██████████| 34/34 [00:00<00:00, 216.43batch/s, acc=1, loss=0.00568]   \n",
      "Epoch 70: 100%|██████████| 34/34 [00:00<00:00, 218.62batch/s, acc=1, loss=0.00543]   \n",
      "Epoch 71: 100%|██████████| 34/34 [00:00<00:00, 80.76batch/s, acc=1, loss=0.00514]    \n",
      "Epoch 72: 100%|██████████| 34/34 [00:00<00:00, 182.49batch/s, acc=1, loss=0.00489]   \n",
      "Epoch 73: 100%|██████████| 34/34 [00:00<00:00, 175.53batch/s, acc=1, loss=0.00466]   \n",
      "Epoch 74: 100%|██████████| 34/34 [00:00<00:00, 171.58batch/s, acc=1, loss=0.00447]   \n",
      "Epoch 75: 100%|██████████| 34/34 [00:00<00:00, 171.80batch/s, acc=1, loss=0.00426]   \n",
      "Epoch 76: 100%|██████████| 34/34 [00:00<00:00, 171.91batch/s, acc=1, loss=0.00401]   \n",
      "Epoch 77: 100%|██████████| 34/34 [00:00<00:00, 159.84batch/s, acc=1, loss=0.00387]   \n",
      "Epoch 78: 100%|██████████| 34/34 [00:00<00:00, 80.73batch/s, acc=1, loss=0.0037]     \n",
      "Epoch 79: 100%|██████████| 34/34 [00:00<00:00, 165.78batch/s, acc=1, loss=0.00353]   \n",
      "Epoch 80: 100%|██████████| 34/34 [00:00<00:00, 160.00batch/s, acc=1, loss=0.0034]    \n",
      "Epoch 81: 100%|██████████| 34/34 [00:00<00:00, 168.65batch/s, acc=1, loss=0.00318]   \n",
      "Epoch 82: 100%|██████████| 34/34 [00:00<00:00, 172.39batch/s, acc=1, loss=0.00313]   \n",
      "Epoch 83: 100%|██████████| 34/34 [00:00<00:00, 67.19batch/s, acc=1, loss=0.00299]    \n",
      "Epoch 84: 100%|██████████| 34/34 [00:00<00:00, 166.45batch/s, acc=1, loss=0.00283]   \n",
      "Epoch 85: 100%|██████████| 34/34 [00:00<00:00, 164.15batch/s, acc=1, loss=0.00269]   \n",
      "Epoch 86: 100%|██████████| 34/34 [00:00<00:00, 133.64batch/s, acc=1, loss=0.00256]   \n",
      "Epoch 87: 100%|██████████| 34/34 [00:00<00:00, 161.64batch/s, acc=1, loss=0.0025]    \n",
      "Epoch 88: 100%|██████████| 34/34 [00:00<00:00, 163.66batch/s, acc=1, loss=0.00234]   \n",
      "Epoch 89: 100%|██████████| 34/34 [00:00<00:00, 89.54batch/s, acc=1, loss=0.00229]    \n",
      "Epoch 90: 100%|██████████| 34/34 [00:00<00:00, 203.14batch/s, acc=1, loss=0.00212]   \n",
      "Epoch 91: 100%|██████████| 34/34 [00:00<00:00, 207.73batch/s, acc=1, loss=0.00209]   \n",
      "Epoch 92: 100%|██████████| 34/34 [00:00<00:00, 207.14batch/s, acc=1, loss=0.00196]   \n",
      "Epoch 93: 100%|██████████| 34/34 [00:00<00:00, 209.30batch/s, acc=1, loss=0.00189]   \n",
      "Epoch 94: 100%|██████████| 34/34 [00:00<00:00, 207.76batch/s, acc=1, loss=0.00179]   \n",
      "Epoch 95: 100%|██████████| 34/34 [00:00<00:00, 84.47batch/s, acc=1, loss=0.00175]    \n",
      "Epoch 96: 100%|██████████| 34/34 [00:00<00:00, 205.83batch/s, acc=1, loss=0.00165]   \n",
      "Epoch 97: 100%|██████████| 34/34 [00:00<00:00, 205.98batch/s, acc=1, loss=0.00158]   \n",
      "Epoch 98: 100%|██████████| 34/34 [00:00<00:00, 207.32batch/s, acc=1, loss=0.00154]   \n",
      "Epoch 99: 100%|██████████| 34/34 [00:00<00:00, 209.84batch/s, acc=1, loss=0.00132]   \n",
      "Epoch 100: 100%|██████████| 34/34 [00:00<00:00, 91.72batch/s, acc=1, loss=0.00138]    \n",
      "Epoch 101: 100%|██████████| 34/34 [00:00<00:00, 175.80batch/s, acc=1, loss=0.00125]   \n",
      "Epoch 102: 100%|██████████| 34/34 [00:00<00:00, 208.52batch/s, acc=1, loss=0.00118]   \n",
      "Epoch 103: 100%|██████████| 34/34 [00:00<00:00, 213.58batch/s, acc=1, loss=0.00129]   \n",
      "Epoch 104: 100%|██████████| 34/34 [00:00<00:00, 202.69batch/s, acc=1, loss=0.00112]   \n",
      "Epoch 105: 100%|██████████| 34/34 [00:00<00:00, 204.00batch/s, acc=1, loss=0.00106]   \n",
      "Epoch 106: 100%|██████████| 34/34 [00:00<00:00, 86.63batch/s, acc=1, loss=0.00104]   \n",
      "Epoch 107: 100%|██████████| 34/34 [00:00<00:00, 171.98batch/s, acc=1, loss=0.00102]   \n",
      "Epoch 108: 100%|██████████| 34/34 [00:00<00:00, 147.17batch/s, acc=1, loss=0.00121]   \n",
      "Epoch 109: 100%|██████████| 34/34 [00:00<00:00, 192.30batch/s, acc=1, loss=0.000957] \n",
      "Epoch 110: 100%|██████████| 34/34 [00:00<00:00, 204.91batch/s, acc=1, loss=0.000916]  \n",
      "Epoch 111: 100%|██████████| 34/34 [00:00<00:00, 94.76batch/s, acc=1, loss=0.000922]  \n",
      "Epoch 112: 100%|██████████| 34/34 [00:00<00:00, 203.06batch/s, acc=1, loss=0.000878]  \n",
      "Epoch 113: 100%|██████████| 34/34 [00:00<00:00, 207.97batch/s, acc=1, loss=0.000859]  \n",
      "Epoch 114: 100%|██████████| 34/34 [00:00<00:00, 213.19batch/s, acc=1, loss=0.000845]  \n",
      "Epoch 115: 100%|██████████| 34/34 [00:00<00:00, 211.29batch/s, acc=1, loss=0.00098]  \n",
      "Epoch 116: 100%|██████████| 34/34 [00:00<00:00, 85.37batch/s, acc=1, loss=0.000803]   \n",
      "Epoch 117: 100%|██████████| 34/34 [00:00<00:00, 207.38batch/s, acc=1, loss=0.000756]  \n",
      "Epoch 118: 100%|██████████| 34/34 [00:00<00:00, 206.38batch/s, acc=1, loss=0.00075]   \n",
      "Epoch 119: 100%|██████████| 34/34 [00:00<00:00, 191.30batch/s, acc=1, loss=0.000731] \n",
      "Epoch 120: 100%|██████████| 34/34 [00:00<00:00, 202.32batch/s, acc=1, loss=0.000708] \n",
      "Epoch 121: 100%|██████████| 34/34 [00:00<00:00, 88.57batch/s, acc=1, loss=0.000697]  \n",
      "Epoch 122: 100%|██████████| 34/34 [00:00<00:00, 218.06batch/s, acc=1, loss=0.000673] \n",
      "Epoch 123: 100%|██████████| 34/34 [00:00<00:00, 206.17batch/s, acc=1, loss=0.000665] \n",
      "Epoch 124: 100%|██████████| 34/34 [00:00<00:00, 166.61batch/s, acc=1, loss=0.000646] \n",
      "Epoch 125: 100%|██████████| 34/34 [00:00<00:00, 85.64batch/s, acc=1, loss=0.000626]  \n",
      "Epoch 126: 100%|██████████| 34/34 [00:00<00:00, 211.85batch/s, acc=1, loss=0.000811]\n",
      "Epoch 127: 100%|██████████| 34/34 [00:00<00:00, 205.31batch/s, acc=1, loss=0.000603]  \n",
      "Epoch 128: 100%|██████████| 34/34 [00:00<00:00, 214.25batch/s, acc=1, loss=0.000581] \n",
      "Epoch 129: 100%|██████████| 34/34 [00:00<00:00, 212.06batch/s, acc=1, loss=0.000577] \n",
      "Epoch 130: 100%|██████████| 34/34 [00:00<00:00, 95.07batch/s, acc=1, loss=0.000551] \n",
      "Epoch 131: 100%|██████████| 34/34 [00:00<00:00, 215.27batch/s, acc=1, loss=0.000543] \n",
      "Epoch 132: 100%|██████████| 34/34 [00:00<00:00, 164.57batch/s, acc=1, loss=0.000535] \n",
      "Epoch 133: 100%|██████████| 34/34 [00:00<00:00, 202.71batch/s, acc=1, loss=0.000517] \n",
      "Epoch 134: 100%|██████████| 34/34 [00:00<00:00, 97.25batch/s, acc=1, loss=0.000507]  \n",
      "Epoch 135: 100%|██████████| 34/34 [00:00<00:00, 162.44batch/s, acc=1, loss=0.000494] \n",
      "Epoch 136: 100%|██████████| 34/34 [00:00<00:00, 190.40batch/s, acc=1, loss=0.000492] \n",
      "Epoch 137: 100%|██████████| 34/34 [00:00<00:00, 189.71batch/s, acc=1, loss=0.000473] \n",
      "Epoch 138: 100%|██████████| 34/34 [00:00<00:00, 158.09batch/s, acc=1, loss=0.000462] \n",
      "Epoch 139: 100%|██████████| 34/34 [00:00<00:00, 84.44batch/s, acc=1, loss=0.000456]  \n",
      "Epoch 140: 100%|██████████| 34/34 [00:00<00:00, 203.00batch/s, acc=1, loss=0.000453] \n",
      "Epoch 141: 100%|██████████| 34/34 [00:00<00:00, 199.46batch/s, acc=1, loss=0.000439] \n",
      "Epoch 142: 100%|██████████| 34/34 [00:00<00:00, 192.59batch/s, acc=1, loss=0.000431] \n",
      "Epoch 143: 100%|██████████| 34/34 [00:00<00:00, 89.52batch/s, acc=1, loss=0.000418] \n",
      "Epoch 144: 100%|██████████| 34/34 [00:00<00:00, 212.57batch/s, acc=1, loss=0.000412] \n",
      "Epoch 145: 100%|██████████| 34/34 [00:00<00:00, 162.62batch/s, acc=1, loss=0.000406] \n",
      "Epoch 146: 100%|██████████| 34/34 [00:00<00:00, 195.77batch/s, acc=1, loss=0.000399] \n",
      "Epoch 147: 100%|██████████| 34/34 [00:00<00:00, 74.34batch/s, acc=1, loss=0.000385] \n",
      "Epoch 148: 100%|██████████| 34/34 [00:00<00:00, 139.30batch/s, acc=1, loss=0.000386] \n",
      "Epoch 149: 100%|██████████| 34/34 [00:00<00:00, 150.04batch/s, acc=1, loss=0.000633] \n",
      "Epoch 150: 100%|██████████| 34/34 [00:00<00:00, 134.03batch/s, acc=1, loss=0.000389] \n",
      "Epoch 151: 100%|██████████| 34/34 [00:00<00:00, 73.30batch/s, acc=1, loss=0.000361] \n",
      "Epoch 152: 100%|██████████| 34/34 [00:00<00:00, 162.75batch/s, acc=1, loss=0.000355] \n",
      "Epoch 153: 100%|██████████| 34/34 [00:00<00:00, 136.03batch/s, acc=1, loss=0.000339] \n",
      "Epoch 154: 100%|██████████| 34/34 [00:00<00:00, 162.40batch/s, acc=1, loss=0.000335] \n",
      "Epoch 155: 100%|██████████| 34/34 [00:00<00:00, 67.19batch/s, acc=1, loss=0.000325] \n",
      "Epoch 156: 100%|██████████| 34/34 [00:00<00:00, 184.61batch/s, acc=1, loss=0.000324] \n",
      "Epoch 157: 100%|██████████| 34/34 [00:00<00:00, 180.43batch/s, acc=1, loss=0.000308] \n",
      "Epoch 158: 100%|██████████| 34/34 [00:00<00:00, 172.99batch/s, acc=1, loss=0.000308] \n",
      "Epoch 159: 100%|██████████| 34/34 [00:00<00:00, 81.09batch/s, acc=1, loss=0.000304]  \n",
      "Epoch 160: 100%|██████████| 34/34 [00:00<00:00, 158.60batch/s, acc=1, loss=0.000294]\n",
      "Epoch 161: 100%|██████████| 34/34 [00:00<00:00, 205.62batch/s, acc=1, loss=0.000285] \n",
      "Epoch 162: 100%|██████████| 34/34 [00:00<00:00, 156.14batch/s, acc=1, loss=0.000281] \n",
      "Epoch 163: 100%|██████████| 34/34 [00:00<00:00, 97.27batch/s, acc=1, loss=0.000275]  \n",
      "Epoch 164: 100%|██████████| 34/34 [00:00<00:00, 202.97batch/s, acc=1, loss=0.000271] \n",
      "Epoch 165: 100%|██████████| 34/34 [00:00<00:00, 200.19batch/s, acc=1, loss=0.000269] \n",
      "Epoch 166: 100%|██████████| 34/34 [00:00<00:00, 89.81batch/s, acc=1, loss=0.000264]  \n",
      "Epoch 167: 100%|██████████| 34/34 [00:00<00:00, 172.12batch/s, acc=1, loss=0.000258] \n",
      "Epoch 168: 100%|██████████| 34/34 [00:00<00:00, 196.09batch/s, acc=1, loss=0.000252] \n",
      "Epoch 169: 100%|██████████| 34/34 [00:00<00:00, 198.26batch/s, acc=1, loss=0.000247] \n",
      "Epoch 170: 100%|██████████| 34/34 [00:00<00:00, 85.33batch/s, acc=1, loss=0.000243] \n",
      "Epoch 171: 100%|██████████| 34/34 [00:00<00:00, 157.71batch/s, acc=1, loss=0.00024]  \n",
      "Epoch 172: 100%|██████████| 34/34 [00:00<00:00, 191.85batch/s, acc=1, loss=0.000233] \n",
      "Epoch 173: 100%|██████████| 34/34 [00:00<00:00, 113.68batch/s, acc=1, loss=0.000229] \n",
      "Epoch 174: 100%|██████████| 34/34 [00:00<00:00, 80.27batch/s, acc=1, loss=0.000228]  \n",
      "Epoch 175: 100%|██████████| 34/34 [00:00<00:00, 111.39batch/s, acc=1, loss=0.000224] \n",
      "Epoch 176: 100%|██████████| 34/34 [00:00<00:00, 158.08batch/s, acc=1, loss=0.000216] \n",
      "Epoch 177: 100%|██████████| 34/34 [00:00<00:00, 44.36batch/s, acc=1, loss=0.000214] \n",
      "Epoch 178: 100%|██████████| 34/34 [00:00<00:00, 88.35batch/s, acc=1, loss=0.000212]  \n",
      "Epoch 179: 100%|██████████| 34/34 [00:00<00:00, 78.32batch/s, acc=1, loss=0.000209] \n",
      "Epoch 180: 100%|██████████| 34/34 [00:00<00:00, 80.06batch/s, acc=1, loss=0.000202]  \n",
      "Epoch 181: 100%|██████████| 34/34 [00:00<00:00, 67.39batch/s, acc=1, loss=0.000202]  \n",
      "Epoch 182: 100%|██████████| 34/34 [00:00<00:00, 136.55batch/s, acc=1, loss=0.000193] \n",
      "Epoch 183: 100%|██████████| 34/34 [00:00<00:00, 137.86batch/s, acc=1, loss=0.000192] \n",
      "Epoch 184: 100%|██████████| 34/34 [00:00<00:00, 82.38batch/s, acc=1, loss=0.000193] \n",
      "Epoch 185: 100%|██████████| 34/34 [00:00<00:00, 124.15batch/s, acc=1, loss=0.000189] \n",
      "Epoch 186: 100%|██████████| 34/34 [00:00<00:00, 159.81batch/s, acc=1, loss=0.000184] \n",
      "Epoch 187: 100%|██████████| 34/34 [00:00<00:00, 68.21batch/s, acc=1, loss=0.000179]  \n",
      "Epoch 188: 100%|██████████| 34/34 [00:00<00:00, 126.24batch/s, acc=1, loss=0.000181] \n",
      "Epoch 189: 100%|██████████| 34/34 [00:00<00:00, 155.45batch/s, acc=1, loss=0.000175] \n",
      "Epoch 190: 100%|██████████| 34/34 [00:00<00:00, 143.74batch/s, acc=1, loss=0.000171] \n",
      "Epoch 191: 100%|██████████| 34/34 [00:00<00:00, 69.51batch/s, acc=1, loss=0.000168]  \n",
      "Epoch 192: 100%|██████████| 34/34 [00:00<00:00, 193.01batch/s, acc=1, loss=0.000168] \n",
      "Epoch 193: 100%|██████████| 34/34 [00:00<00:00, 190.23batch/s, acc=1, loss=0.000167] \n",
      "Epoch 194: 100%|██████████| 34/34 [00:00<00:00, 84.91batch/s, acc=1, loss=0.000161]  \n",
      "Epoch 195: 100%|██████████| 34/34 [00:00<00:00, 155.58batch/s, acc=1, loss=0.000157] \n",
      "Epoch 196: 100%|██████████| 34/34 [00:00<00:00, 143.63batch/s, acc=1, loss=0.000155] \n",
      "Epoch 197: 100%|██████████| 34/34 [00:00<00:00, 88.64batch/s, acc=1, loss=0.000153] \n",
      "Epoch 198: 100%|██████████| 34/34 [00:00<00:00, 206.09batch/s, acc=1, loss=0.000151] \n",
      "Epoch 199: 100%|██████████| 34/34 [00:00<00:00, 183.56batch/s, acc=1, loss=0.000149] \n",
      "Epoch 200: 100%|██████████| 34/34 [00:00<00:00, 84.40batch/s, acc=1, loss=0.000147]  \n",
      "Epoch 201: 100%|██████████| 34/34 [00:00<00:00, 155.68batch/s, acc=1, loss=0.000146] \n",
      "Epoch 202: 100%|██████████| 34/34 [00:00<00:00, 169.63batch/s, acc=1, loss=0.000141] \n",
      "Epoch 203: 100%|██████████| 34/34 [00:00<00:00, 169.54batch/s, acc=1, loss=0.000138] \n",
      "Epoch 204: 100%|██████████| 34/34 [00:00<00:00, 99.66batch/s, acc=1, loss=0.000138]  \n",
      "Epoch 205: 100%|██████████| 34/34 [00:00<00:00, 184.67batch/s, acc=1, loss=0.000134] \n",
      "Epoch 206: 100%|██████████| 34/34 [00:00<00:00, 184.69batch/s, acc=1, loss=0.000133] \n",
      "Epoch 207: 100%|██████████| 34/34 [00:00<00:00, 71.55batch/s, acc=1, loss=0.000131] \n",
      "Epoch 208: 100%|██████████| 34/34 [00:00<00:00, 144.07batch/s, acc=1, loss=0.000131] \n",
      "Epoch 209: 100%|██████████| 34/34 [00:00<00:00, 161.25batch/s, acc=1, loss=0.000959] \n",
      "Epoch 210: 100%|██████████| 34/34 [00:00<00:00, 62.10batch/s, acc=1, loss=0.00016]  \n",
      "Epoch 211: 100%|██████████| 34/34 [00:00<00:00, 128.03batch/s, acc=1, loss=0.000139]\n",
      "Epoch 212: 100%|██████████| 34/34 [00:00<00:00, 178.54batch/s, acc=1, loss=0.000135] \n",
      "Epoch 213: 100%|██████████| 34/34 [00:00<00:00, 86.16batch/s, acc=1, loss=0.000127] \n",
      "Epoch 214: 100%|██████████| 34/34 [00:00<00:00, 187.04batch/s, acc=1, loss=0.000119] \n",
      "Epoch 215: 100%|██████████| 34/34 [00:00<00:00, 162.84batch/s, acc=1, loss=0.000117] \n",
      "Epoch 216: 100%|██████████| 34/34 [00:00<00:00, 96.87batch/s, acc=1, loss=0.000113]  \n",
      "Epoch 217: 100%|██████████| 34/34 [00:00<00:00, 166.62batch/s, acc=1, loss=0.000112] \n",
      "Epoch 218: 100%|██████████| 34/34 [00:00<00:00, 203.35batch/s, acc=1, loss=0.000107] \n",
      "Epoch 219: 100%|██████████| 34/34 [00:00<00:00, 84.13batch/s, acc=1, loss=0.000107]  \n",
      "Epoch 220: 100%|██████████| 34/34 [00:00<00:00, 167.88batch/s, acc=1, loss=0.000105] \n",
      "Epoch 221: 100%|██████████| 34/34 [00:00<00:00, 189.96batch/s, acc=1, loss=0.000103] \n",
      "Epoch 222: 100%|██████████| 34/34 [00:00<00:00, 87.31batch/s, acc=1, loss=9.96e-5]   \n",
      "Epoch 223: 100%|██████████| 34/34 [00:00<00:00, 150.24batch/s, acc=1, loss=9.8e-5]  \n",
      "Epoch 224: 100%|██████████| 34/34 [00:00<00:00, 141.47batch/s, acc=1, loss=9.65e-5]  \n",
      "Epoch 225: 100%|██████████| 34/34 [00:00<00:00, 74.35batch/s, acc=1, loss=9.42e-5]  \n",
      "Epoch 226: 100%|██████████| 34/34 [00:00<00:00, 196.70batch/s, acc=1, loss=9.19e-5]  \n",
      "Epoch 227: 100%|██████████| 34/34 [00:00<00:00, 145.93batch/s, acc=1, loss=9.22e-5]  \n",
      "Epoch 228: 100%|██████████| 34/34 [00:00<00:00, 82.77batch/s, acc=1, loss=8.97e-5]   \n",
      "Epoch 229: 100%|██████████| 34/34 [00:00<00:00, 187.95batch/s, acc=1, loss=8.72e-5]  \n",
      "Epoch 230: 100%|██████████| 34/34 [00:00<00:00, 191.44batch/s, acc=1, loss=8.63e-5]  \n",
      "Epoch 231: 100%|██████████| 34/34 [00:00<00:00, 75.12batch/s, acc=1, loss=8.61e-5]   \n",
      "Epoch 232: 100%|██████████| 34/34 [00:00<00:00, 188.62batch/s, acc=1, loss=8.35e-5]  \n",
      "Epoch 233: 100%|██████████| 34/34 [00:00<00:00, 196.81batch/s, acc=1, loss=8.14e-5]  \n",
      "Epoch 234: 100%|██████████| 34/34 [00:00<00:00, 74.42batch/s, acc=1, loss=8e-5]      \n",
      "Epoch 235: 100%|██████████| 34/34 [00:00<00:00, 187.17batch/s, acc=1, loss=8.12e-5]  \n",
      "Epoch 236: 100%|██████████| 34/34 [00:00<00:00, 91.12batch/s, acc=1, loss=7.86e-5]   \n",
      "Epoch 237: 100%|██████████| 34/34 [00:00<00:00, 188.01batch/s, acc=1, loss=7.83e-5]  \n",
      "Epoch 238: 100%|██████████| 34/34 [00:00<00:00, 160.76batch/s, acc=1, loss=7.48e-5]  \n",
      "Epoch 239: 100%|██████████| 34/34 [00:00<00:00, 91.70batch/s, acc=1, loss=7.38e-5]   \n",
      "Epoch 240: 100%|██████████| 34/34 [00:00<00:00, 193.42batch/s, acc=1, loss=7.51e-5]  \n",
      "Epoch 241: 100%|██████████| 34/34 [00:00<00:00, 195.49batch/s, acc=1, loss=7.32e-5]  \n",
      "Epoch 242: 100%|██████████| 34/34 [00:00<00:00, 79.29batch/s, acc=1, loss=7.13e-5]  \n",
      "Epoch 243: 100%|██████████| 34/34 [00:00<00:00, 200.50batch/s, acc=1, loss=6.9e-5]   \n",
      "Epoch 244: 100%|██████████| 34/34 [00:00<00:00, 197.11batch/s, acc=1, loss=7e-5]     \n",
      "Epoch 245: 100%|██████████| 34/34 [00:00<00:00, 83.71batch/s, acc=1, loss=6.84e-5]  \n",
      "Epoch 246: 100%|██████████| 34/34 [00:00<00:00, 204.35batch/s, acc=1, loss=6.79e-5]  \n",
      "Epoch 247: 100%|██████████| 34/34 [00:00<00:00, 196.18batch/s, acc=1, loss=6.51e-5]  \n",
      "Epoch 248: 100%|██████████| 34/34 [00:00<00:00, 89.93batch/s, acc=1, loss=6.6e-5]    \n",
      "Epoch 249: 100%|██████████| 34/34 [00:00<00:00, 157.89batch/s, acc=1, loss=6.49e-5] \n",
      "Epoch 250: 100%|██████████| 34/34 [00:00<00:00, 184.11batch/s, acc=1, loss=6.36e-5]  \n",
      "Epoch 251: 100%|██████████| 34/34 [00:00<00:00, 88.36batch/s, acc=1, loss=6.18e-5]   \n",
      "Epoch 252: 100%|██████████| 34/34 [00:00<00:00, 187.68batch/s, acc=1, loss=6.21e-5]  \n",
      "Epoch 253: 100%|██████████| 34/34 [00:00<00:00, 80.24batch/s, acc=1, loss=6.1e-5]    \n",
      "Epoch 254: 100%|██████████| 34/34 [00:00<00:00, 177.72batch/s, acc=1, loss=6.08e-5]  \n",
      "Epoch 255: 100%|██████████| 34/34 [00:00<00:00, 192.31batch/s, acc=1, loss=5.95e-5]  \n",
      "Epoch 256: 100%|██████████| 34/34 [00:00<00:00, 80.30batch/s, acc=1, loss=5.79e-5]  \n",
      "Epoch 257: 100%|██████████| 34/34 [00:00<00:00, 191.78batch/s, acc=1, loss=5.79e-5]  \n",
      "Epoch 258: 100%|██████████| 34/34 [00:00<00:00, 114.49batch/s, acc=1, loss=5.73e-5]  \n",
      "Epoch 259: 100%|██████████| 34/34 [00:00<00:00, 112.34batch/s, acc=1, loss=5.61e-5]  \n",
      "Epoch 260: 100%|██████████| 34/34 [00:00<00:00, 203.43batch/s, acc=1, loss=5.44e-5]  \n",
      "Epoch 261: 100%|██████████| 34/34 [00:00<00:00, 89.90batch/s, acc=1, loss=5.55e-5]  \n",
      "Epoch 262: 100%|██████████| 34/34 [00:00<00:00, 209.11batch/s, acc=1, loss=5.38e-5]  \n",
      "Epoch 263: 100%|██████████| 34/34 [00:00<00:00, 163.80batch/s, acc=1, loss=5.21e-5]  \n",
      "Epoch 264: 100%|██████████| 34/34 [00:00<00:00, 89.43batch/s, acc=1, loss=5.04e-5]   \n",
      "Epoch 265: 100%|██████████| 34/34 [00:00<00:00, 81.59batch/s, acc=1, loss=5.27e-5]   \n",
      "Epoch 266: 100%|██████████| 34/34 [00:00<00:00, 78.03batch/s, acc=1, loss=5.21e-5]   \n",
      "Epoch 267: 100%|██████████| 34/34 [00:00<00:00, 146.92batch/s, acc=1, loss=4.93e-5]  \n",
      "Epoch 268: 100%|██████████| 34/34 [00:00<00:00, 144.56batch/s, acc=1, loss=4.99e-5]  \n",
      "Epoch 269: 100%|██████████| 34/34 [00:00<00:00, 89.70batch/s, acc=1, loss=4.87e-5]  \n",
      "Epoch 270: 100%|██████████| 34/34 [00:00<00:00, 200.74batch/s, acc=1, loss=4.83e-5]  \n",
      "Epoch 271: 100%|██████████| 34/34 [00:00<00:00, 191.71batch/s, acc=1, loss=4.74e-5]  \n",
      "Epoch 272: 100%|██████████| 34/34 [00:00<00:00, 81.19batch/s, acc=1, loss=4.69e-5]   \n",
      "Epoch 273: 100%|██████████| 34/34 [00:00<00:00, 164.42batch/s, acc=1, loss=4.56e-5]  \n",
      "Epoch 274: 100%|██████████| 34/34 [00:00<00:00, 52.71batch/s, acc=1, loss=4.5e-5]   \n",
      "Epoch 275: 100%|██████████| 34/34 [00:00<00:00, 104.24batch/s, acc=1, loss=4.47e-5]  \n",
      "Epoch 276: 100%|██████████| 34/34 [00:00<00:00, 149.08batch/s, acc=1, loss=4.47e-5]  \n",
      "Epoch 277: 100%|██████████| 34/34 [00:00<00:00, 83.11batch/s, acc=1, loss=4.34e-5]   \n",
      "Epoch 278: 100%|██████████| 34/34 [00:00<00:00, 149.71batch/s, acc=1, loss=4.26e-5]  \n",
      "Epoch 279: 100%|██████████| 34/34 [00:00<00:00, 78.15batch/s, acc=1, loss=4.25e-5]  \n",
      "Epoch 280: 100%|██████████| 34/34 [00:00<00:00, 153.85batch/s, acc=1, loss=4.19e-5]  \n",
      "Epoch 281: 100%|██████████| 34/34 [00:00<00:00, 201.28batch/s, acc=1, loss=4.09e-5]  \n",
      "Epoch 282: 100%|██████████| 34/34 [00:00<00:00, 87.11batch/s, acc=1, loss=4.05e-5]   \n",
      "Epoch 283: 100%|██████████| 34/34 [00:00<00:00, 160.17batch/s, acc=1, loss=3.99e-5] \n",
      "Epoch 284: 100%|██████████| 34/34 [00:00<00:00, 91.37batch/s, acc=1, loss=3.97e-5]   \n",
      "Epoch 285: 100%|██████████| 34/34 [00:00<00:00, 201.10batch/s, acc=1, loss=3.92e-5]  \n",
      "Epoch 286: 100%|██████████| 34/34 [00:00<00:00, 166.02batch/s, acc=1, loss=3.87e-5]  \n",
      "Epoch 287: 100%|██████████| 34/34 [00:00<00:00, 91.83batch/s, acc=1, loss=3.76e-5]   \n",
      "Epoch 288: 100%|██████████| 34/34 [00:00<00:00, 195.82batch/s, acc=1, loss=3.67e-5]  \n",
      "Epoch 289: 100%|██████████| 34/34 [00:00<00:00, 173.15batch/s, acc=1, loss=3.77e-5]  \n",
      "Epoch 290: 100%|██████████| 34/34 [00:00<00:00, 84.55batch/s, acc=1, loss=3.58e-5]   \n",
      "Epoch 291: 100%|██████████| 34/34 [00:00<00:00, 189.96batch/s, acc=1, loss=3.54e-5]  \n",
      "Epoch 292: 100%|██████████| 34/34 [00:00<00:00, 73.05batch/s, acc=1, loss=3.53e-5]   \n",
      "Epoch 293: 100%|██████████| 34/34 [00:00<00:00, 170.70batch/s, acc=1, loss=3.55e-5]  \n",
      "Epoch 294: 100%|██████████| 34/34 [00:00<00:00, 97.72batch/s, acc=1, loss=3.42e-5]   \n",
      "Epoch 295: 100%|██████████| 34/34 [00:00<00:00, 129.99batch/s, acc=1, loss=3.39e-5]  \n",
      "Epoch 296: 100%|██████████| 34/34 [00:00<00:00, 199.49batch/s, acc=1, loss=3.3e-5]   \n",
      "Epoch 297: 100%|██████████| 34/34 [00:00<00:00, 89.02batch/s, acc=1, loss=3.27e-5]   \n",
      "Epoch 298: 100%|██████████| 34/34 [00:00<00:00, 167.09batch/s, acc=1, loss=3.25e-5]  \n",
      "Epoch 299: 100%|██████████| 34/34 [00:00<00:00, 90.43batch/s, acc=1, loss=3.18e-5]  \n",
      "Epoch 300: 100%|██████████| 34/34 [00:00<00:00, 197.09batch/s, acc=1, loss=3.19e-5]  \n",
      "Epoch 301: 100%|██████████| 34/34 [00:00<00:00, 153.24batch/s, acc=1, loss=3.15e-5]  \n",
      "Epoch 302: 100%|██████████| 34/34 [00:00<00:00, 120.57batch/s, acc=1, loss=3.02e-5]  \n",
      "Epoch 303: 100%|██████████| 34/34 [00:00<00:00, 187.19batch/s, acc=1, loss=2.99e-5]  \n",
      "Epoch 304: 100%|██████████| 34/34 [00:00<00:00, 74.22batch/s, acc=1, loss=3.05e-5]  \n",
      "Epoch 305: 100%|██████████| 34/34 [00:00<00:00, 177.37batch/s, acc=1, loss=2.95e-5]  \n",
      "Epoch 306: 100%|██████████| 34/34 [00:00<00:00, 66.45batch/s, acc=1, loss=2.9e-5]   \n",
      "Epoch 307: 100%|██████████| 34/34 [00:00<00:00, 130.97batch/s, acc=1, loss=2.76e-5]  \n",
      "Epoch 308: 100%|██████████| 34/34 [00:00<00:00, 76.65batch/s, acc=1, loss=2.86e-5]   \n",
      "Epoch 309: 100%|██████████| 34/34 [00:00<00:00, 115.24batch/s, acc=1, loss=2.79e-5]  \n",
      "Epoch 310: 100%|██████████| 34/34 [00:00<00:00, 108.88batch/s, acc=1, loss=2.71e-5]  \n",
      "Epoch 311: 100%|██████████| 34/34 [00:00<00:00, 96.49batch/s, acc=1, loss=2.66e-5]   \n",
      "Epoch 312: 100%|██████████| 34/34 [00:00<00:00, 134.32batch/s, acc=1, loss=2.71e-5]  \n",
      "Epoch 313: 100%|██████████| 34/34 [00:00<00:00, 54.10batch/s, acc=1, loss=2.65e-5]   \n",
      "Epoch 314: 100%|██████████| 34/34 [00:00<00:00, 126.66batch/s, acc=1, loss=2.55e-5]  \n",
      "Epoch 315: 100%|██████████| 34/34 [00:00<00:00, 58.10batch/s, acc=1, loss=2.52e-5]  \n",
      "Epoch 316: 100%|██████████| 34/34 [00:00<00:00, 128.91batch/s, acc=1, loss=2.49e-5]  \n",
      "Epoch 317: 100%|██████████| 34/34 [00:00<00:00, 63.69batch/s, acc=1, loss=2.47e-5]   \n",
      "Epoch 318: 100%|██████████| 34/34 [00:00<00:00, 76.64batch/s, acc=1, loss=2.4e-5]   \n",
      "Epoch 319: 100%|██████████| 34/34 [00:00<00:00, 47.54batch/s, acc=1, loss=2.4e-5]   \n",
      "Epoch 320: 100%|██████████| 34/34 [00:00<00:00, 90.57batch/s, acc=1, loss=2.39e-5]   \n",
      "Epoch 321: 100%|██████████| 34/34 [00:00<00:00, 94.91batch/s, acc=1, loss=2.31e-5]  \n",
      "Epoch 322: 100%|██████████| 34/34 [00:00<00:00, 47.62batch/s, acc=1, loss=2.25e-5]  \n",
      "Epoch 323: 100%|██████████| 34/34 [00:00<00:00, 86.49batch/s, acc=1, loss=2.29e-5]   \n",
      "Epoch 324: 100%|██████████| 34/34 [00:00<00:00, 48.62batch/s, acc=1, loss=2.26e-5]  \n",
      "Epoch 325: 100%|██████████| 34/34 [00:00<00:00, 104.50batch/s, acc=1, loss=2.21e-5]  \n",
      "Epoch 326: 100%|██████████| 34/34 [00:00<00:00, 50.29batch/s, acc=1, loss=2.13e-5]  \n",
      "Epoch 327: 100%|██████████| 34/34 [00:00<00:00, 105.83batch/s, acc=1, loss=2.07e-5]  \n",
      "Epoch 328: 100%|██████████| 34/34 [00:00<00:00, 46.72batch/s, acc=1, loss=2.12e-5]   \n",
      "Epoch 329: 100%|██████████| 34/34 [00:00<00:00, 76.19batch/s, acc=1, loss=2.07e-5]  \n",
      "Epoch 330: 100%|██████████| 34/34 [00:00<00:00, 56.48batch/s, acc=1, loss=2.02e-5]   \n",
      "Epoch 331: 100%|██████████| 34/34 [00:00<00:00, 68.38batch/s, acc=1, loss=1.97e-5]   \n",
      "Epoch 332: 100%|██████████| 34/34 [00:00<00:00, 77.89batch/s, acc=1, loss=1.96e-5]   \n",
      "Epoch 333: 100%|██████████| 34/34 [00:00<00:00, 58.31batch/s, acc=1, loss=1.95e-5]   \n",
      "Epoch 334: 100%|██████████| 34/34 [00:00<00:00, 110.86batch/s, acc=1, loss=1.95e-5]  \n",
      "Epoch 335: 100%|██████████| 34/34 [00:00<00:00, 46.05batch/s, acc=1, loss=1.88e-5]  \n",
      "Epoch 336: 100%|██████████| 34/34 [00:00<00:00, 83.15batch/s, acc=1, loss=1.81e-5]   \n",
      "Epoch 337: 100%|██████████| 34/34 [00:00<00:00, 45.94batch/s, acc=1, loss=1.85e-5]  \n",
      "Epoch 338: 100%|██████████| 34/34 [00:00<00:00, 71.62batch/s, acc=1, loss=1.81e-5]   \n",
      "Epoch 339: 100%|██████████| 34/34 [00:00<00:00, 43.14batch/s, acc=1, loss=1.74e-5]  \n",
      "Epoch 340: 100%|██████████| 34/34 [00:00<00:00, 115.85batch/s, acc=1, loss=1.75e-5]  \n",
      "Epoch 341: 100%|██████████| 34/34 [00:00<00:00, 50.87batch/s, acc=1, loss=1.73e-5]  \n",
      "Epoch 342: 100%|██████████| 34/34 [00:00<00:00, 122.99batch/s, acc=1, loss=1.71e-5]  \n",
      "Epoch 343: 100%|██████████| 34/34 [00:00<00:00, 55.19batch/s, acc=1, loss=1.66e-5]   \n",
      "Epoch 344: 100%|██████████| 34/34 [00:00<00:00, 101.73batch/s, acc=1, loss=1.62e-5]  \n",
      "Epoch 345: 100%|██████████| 34/34 [00:00<00:00, 79.40batch/s, acc=1, loss=1.63e-5]   \n",
      "Epoch 346: 100%|██████████| 34/34 [00:00<00:00, 73.05batch/s, acc=1, loss=1.6e-5]   \n",
      "Epoch 347: 100%|██████████| 34/34 [00:00<00:00, 126.38batch/s, acc=1, loss=1.6e-5]   \n",
      "Epoch 348: 100%|██████████| 34/34 [00:00<00:00, 55.06batch/s, acc=1, loss=1.51e-5]  \n",
      "Epoch 349: 100%|██████████| 34/34 [00:00<00:00, 133.90batch/s, acc=1, loss=1.52e-5]  \n",
      "Epoch 350: 100%|██████████| 34/34 [00:00<00:00, 62.04batch/s, acc=1, loss=1.5e-5]   \n",
      "Epoch 351: 100%|██████████| 34/34 [00:00<00:00, 135.29batch/s, acc=1, loss=1.46e-5]  \n",
      "Epoch 352: 100%|██████████| 34/34 [00:00<00:00, 86.21batch/s, acc=1, loss=1.46e-5]   \n",
      "Epoch 353: 100%|██████████| 34/34 [00:00<00:00, 112.78batch/s, acc=1, loss=1.41e-5]  \n",
      "Epoch 354: 100%|██████████| 34/34 [00:00<00:00, 131.91batch/s, acc=1, loss=1.43e-5]  \n",
      "Epoch 355: 100%|██████████| 34/34 [00:00<00:00, 76.53batch/s, acc=1, loss=1.38e-5]   \n",
      "Epoch 356: 100%|██████████| 34/34 [00:00<00:00, 130.82batch/s, acc=1, loss=1.35e-5]  \n",
      "Epoch 357: 100%|██████████| 34/34 [00:00<00:00, 130.32batch/s, acc=1, loss=1.35e-5]  \n",
      "Epoch 358: 100%|██████████| 34/34 [00:00<00:00, 84.52batch/s, acc=1, loss=1.31e-5]   \n",
      "Epoch 359: 100%|██████████| 34/34 [00:00<00:00, 143.23batch/s, acc=1, loss=1.3e-5]   \n",
      "Epoch 360: 100%|██████████| 34/34 [00:00<00:00, 84.96batch/s, acc=1, loss=1.26e-5]   \n",
      "Epoch 361: 100%|██████████| 34/34 [00:00<00:00, 178.89batch/s, acc=1, loss=1.24e-5]  \n",
      "Epoch 362: 100%|██████████| 34/34 [00:00<00:00, 77.24batch/s, acc=1, loss=1.24e-5]   \n",
      "Epoch 363: 100%|██████████| 34/34 [00:00<00:00, 187.57batch/s, acc=1, loss=1.22e-5]  \n",
      "Epoch 364: 100%|██████████| 34/34 [00:00<00:00, 73.81batch/s, acc=1, loss=1.2e-5]   \n",
      "Epoch 365: 100%|██████████| 34/34 [00:00<00:00, 182.03batch/s, acc=1, loss=1.15e-5]  \n",
      "Epoch 366: 100%|██████████| 34/34 [00:00<00:00, 153.13batch/s, acc=1, loss=1.15e-5]  \n",
      "Epoch 367: 100%|██████████| 34/34 [00:00<00:00, 83.87batch/s, acc=1, loss=1.17e-5]   \n",
      "Epoch 368: 100%|██████████| 34/34 [00:00<00:00, 153.75batch/s, acc=1, loss=1.14e-5]  \n",
      "Epoch 369: 100%|██████████| 34/34 [00:00<00:00, 88.86batch/s, acc=1, loss=1.08e-5]  \n",
      "Epoch 370: 100%|██████████| 34/34 [00:00<00:00, 207.13batch/s, acc=1, loss=1.09e-5]  \n",
      "Epoch 371: 100%|██████████| 34/34 [00:00<00:00, 79.79batch/s, acc=1, loss=1.06e-5]  \n",
      "Epoch 372: 100%|██████████| 34/34 [00:00<00:00, 198.70batch/s, acc=1, loss=1.05e-5]  \n",
      "Epoch 373: 100%|██████████| 34/34 [00:00<00:00, 80.12batch/s, acc=1, loss=1.04e-5]   \n",
      "Epoch 374: 100%|██████████| 34/34 [00:00<00:00, 199.07batch/s, acc=1, loss=1.02e-5]  \n",
      "Epoch 375: 100%|██████████| 34/34 [00:00<00:00, 84.96batch/s, acc=1, loss=1.01e-5]   \n",
      "Epoch 376: 100%|██████████| 34/34 [00:00<00:00, 190.34batch/s, acc=1, loss=9.99e-6]  \n",
      "Epoch 377: 100%|██████████| 34/34 [00:00<00:00, 85.42batch/s, acc=1, loss=9.61e-6]   \n",
      "Epoch 378: 100%|██████████| 34/34 [00:00<00:00, 208.46batch/s, acc=1, loss=9.46e-6]  \n",
      "Epoch 379: 100%|██████████| 34/34 [00:00<00:00, 88.91batch/s, acc=1, loss=9.49e-6]   \n",
      "Epoch 380: 100%|██████████| 34/34 [00:00<00:00, 109.29batch/s, acc=1, loss=9.44e-6]  \n",
      "Epoch 381: 100%|██████████| 34/34 [00:00<00:00, 133.99batch/s, acc=1, loss=9.02e-6]  \n",
      "Epoch 382: 100%|██████████| 34/34 [00:00<00:00, 85.65batch/s, acc=1, loss=8.73e-6]   \n",
      "Epoch 383: 100%|██████████| 34/34 [00:00<00:00, 173.53batch/s, acc=1, loss=9.01e-6]  \n",
      "Epoch 384: 100%|██████████| 34/34 [00:00<00:00, 84.41batch/s, acc=1, loss=8.77e-6]  \n",
      "Epoch 385: 100%|██████████| 34/34 [00:00<00:00, 202.97batch/s, acc=1, loss=8.51e-6]  \n",
      "Epoch 386: 100%|██████████| 34/34 [00:00<00:00, 68.51batch/s, acc=1, loss=8.34e-6]  \n",
      "Epoch 387: 100%|██████████| 34/34 [00:00<00:00, 148.91batch/s, acc=1, loss=8.23e-6]  \n",
      "Epoch 388: 100%|██████████| 34/34 [00:00<00:00, 94.15batch/s, acc=1, loss=8.29e-6]  \n",
      "Epoch 389: 100%|██████████| 34/34 [00:00<00:00, 157.86batch/s, acc=1, loss=7.99e-6]  \n",
      "Epoch 390: 100%|██████████| 34/34 [00:00<00:00, 106.86batch/s, acc=1, loss=7.85e-6]  \n",
      "Epoch 391: 100%|██████████| 34/34 [00:00<00:00, 132.16batch/s, acc=1, loss=7.73e-6]  \n",
      "Epoch 392: 100%|██████████| 34/34 [00:00<00:00, 94.60batch/s, acc=1, loss=7.76e-6]   \n",
      "Epoch 393: 100%|██████████| 34/34 [00:00<00:00, 190.65batch/s, acc=1, loss=7.43e-6]  \n",
      "Epoch 394: 100%|██████████| 34/34 [00:00<00:00, 90.73batch/s, acc=1, loss=7.25e-6]  \n",
      "Epoch 395: 100%|██████████| 34/34 [00:00<00:00, 202.38batch/s, acc=1, loss=7.19e-6]  \n",
      "Epoch 396: 100%|██████████| 34/34 [00:00<00:00, 64.53batch/s, acc=1, loss=7.19e-6]  \n",
      "Epoch 397: 100%|██████████| 34/34 [00:00<00:00, 128.00batch/s, acc=1, loss=7.03e-6]  \n",
      "Epoch 398: 100%|██████████| 34/34 [00:00<00:00, 184.08batch/s, acc=1, loss=6.99e-6]  \n",
      "Epoch 399: 100%|██████████| 34/34 [00:00<00:00, 66.40batch/s, acc=1, loss=6.78e-6]  \n",
      "Epoch 400: 100%|██████████| 34/34 [00:00<00:00, 128.22batch/s, acc=1, loss=6.63e-6]  \n",
      "Epoch 401: 100%|██████████| 34/34 [00:00<00:00, 78.01batch/s, acc=1, loss=6.58e-6]  \n",
      "Epoch 402: 100%|██████████| 34/34 [00:00<00:00, 152.45batch/s, acc=1, loss=6.44e-6]  \n",
      "Epoch 403: 100%|██████████| 34/34 [00:00<00:00, 67.16batch/s, acc=1, loss=6.26e-6]   \n",
      "Epoch 404: 100%|██████████| 34/34 [00:00<00:00, 147.86batch/s, acc=1, loss=6.23e-6]  \n",
      "Epoch 405: 100%|██████████| 34/34 [00:00<00:00, 66.43batch/s, acc=1, loss=6.22e-6]   \n",
      "Epoch 406: 100%|██████████| 34/34 [00:00<00:00, 138.63batch/s, acc=1, loss=6.03e-6] \n",
      "Epoch 407: 100%|██████████| 34/34 [00:00<00:00, 64.90batch/s, acc=1, loss=5.96e-6]   \n",
      "Epoch 408: 100%|██████████| 34/34 [00:00<00:00, 56.25batch/s, acc=1, loss=5.86e-6]  \n",
      "Epoch 409: 100%|██████████| 34/34 [00:00<00:00, 118.01batch/s, acc=1, loss=5.73e-6]  \n",
      "Epoch 410: 100%|██████████| 34/34 [00:00<00:00, 100.36batch/s, acc=1, loss=5.75e-6]  \n",
      "Epoch 411: 100%|██████████| 34/34 [00:00<00:00, 73.83batch/s, acc=1, loss=5.55e-6]   \n",
      "Epoch 412: 100%|██████████| 34/34 [00:00<00:00, 90.73batch/s, acc=1, loss=5.55e-6]  \n",
      "Epoch 413: 100%|██████████| 34/34 [00:00<00:00, 101.29batch/s, acc=1, loss=5.34e-6]  \n",
      "Epoch 414: 100%|██████████| 34/34 [00:00<00:00, 69.06batch/s, acc=1, loss=5.3e-6]   \n",
      "Epoch 415: 100%|██████████| 34/34 [00:00<00:00, 144.66batch/s, acc=1, loss=5.26e-6]  \n",
      "Epoch 416: 100%|██████████| 34/34 [00:00<00:00, 72.86batch/s, acc=1, loss=5.29e-6]   \n",
      "Epoch 417: 100%|██████████| 34/34 [00:00<00:00, 75.41batch/s, acc=1, loss=5.04e-6]   \n",
      "Epoch 418: 100%|██████████| 34/34 [00:00<00:00, 108.15batch/s, acc=1, loss=4.95e-6]  \n",
      "Epoch 419: 100%|██████████| 34/34 [00:00<00:00, 63.19batch/s, acc=1, loss=4.84e-6]   \n",
      "Epoch 420: 100%|██████████| 34/34 [00:00<00:00, 116.85batch/s, acc=1, loss=4.87e-6]  \n",
      "Epoch 421: 100%|██████████| 34/34 [00:00<00:00, 69.47batch/s, acc=1, loss=4.73e-6]  \n",
      "Epoch 422: 100%|██████████| 34/34 [00:00<00:00, 158.26batch/s, acc=1, loss=4.66e-6]  \n",
      "Epoch 423: 100%|██████████| 34/34 [00:00<00:00, 67.51batch/s, acc=1, loss=4.57e-6]  \n",
      "Epoch 424: 100%|██████████| 34/34 [00:00<00:00, 99.02batch/s, acc=1, loss=4.54e-6]   \n",
      "Epoch 425: 100%|██████████| 34/34 [00:00<00:00, 94.58batch/s, acc=1, loss=4.32e-6]  \n",
      "Epoch 426: 100%|██████████| 34/34 [00:00<00:00, 52.13batch/s, acc=1, loss=4.33e-6]   \n",
      "Epoch 427: 100%|██████████| 34/34 [00:01<00:00, 28.38batch/s, acc=1, loss=4.31e-6]  \n",
      "Epoch 428: 100%|██████████| 34/34 [00:00<00:00, 43.85batch/s, acc=1, loss=4.16e-6]  \n",
      "Epoch 429: 100%|██████████| 34/34 [00:00<00:00, 55.94batch/s, acc=1, loss=4.19e-6]  \n",
      "Epoch 430: 100%|██████████| 34/34 [00:00<00:00, 65.53batch/s, acc=1, loss=4e-6]     \n",
      "Epoch 431: 100%|██████████| 34/34 [00:00<00:00, 133.28batch/s, acc=1, loss=4e-6]     \n",
      "Epoch 432: 100%|██████████| 34/34 [00:00<00:00, 68.89batch/s, acc=1, loss=3.87e-6]  \n",
      "Epoch 433: 100%|██████████| 34/34 [00:00<00:00, 146.67batch/s, acc=1, loss=3.87e-6]  \n",
      "Epoch 434: 100%|██████████| 34/34 [00:00<00:00, 65.76batch/s, acc=1, loss=3.86e-6]  \n",
      "Epoch 435: 100%|██████████| 34/34 [00:00<00:00, 127.16batch/s, acc=1, loss=3.68e-6]  \n",
      "Epoch 436: 100%|██████████| 34/34 [00:00<00:00, 77.92batch/s, acc=1, loss=3.72e-6]   \n",
      "Epoch 437: 100%|██████████| 34/34 [00:00<00:00, 123.66batch/s, acc=1, loss=3.55e-6]  \n",
      "Epoch 438: 100%|██████████| 34/34 [00:00<00:00, 72.01batch/s, acc=1, loss=3.63e-6]   \n",
      "Epoch 439: 100%|██████████| 34/34 [00:00<00:00, 118.96batch/s, acc=1, loss=3.46e-6]  \n",
      "Epoch 440: 100%|██████████| 34/34 [00:00<00:00, 72.58batch/s, acc=1, loss=3.42e-6]  \n",
      "Epoch 441: 100%|██████████| 34/34 [00:00<00:00, 58.72batch/s, acc=1, loss=3.28e-6]   \n",
      "Epoch 442: 100%|██████████| 34/34 [00:00<00:00, 89.84batch/s, acc=1, loss=3.37e-6]  \n",
      "Epoch 443: 100%|██████████| 34/34 [00:00<00:00, 73.64batch/s, acc=1, loss=3.29e-6]  \n",
      "Epoch 444: 100%|██████████| 34/34 [00:00<00:00, 74.01batch/s, acc=1, loss=3.19e-6]   \n",
      "Epoch 445: 100%|██████████| 34/34 [00:00<00:00, 57.83batch/s, acc=1, loss=3.1e-6]    \n",
      "Epoch 446: 100%|██████████| 34/34 [00:00<00:00, 68.38batch/s, acc=1, loss=3.04e-6]  \n",
      "Epoch 447: 100%|██████████| 34/34 [00:00<00:00, 52.51batch/s, acc=1, loss=3.04e-6]  \n",
      "Epoch 448: 100%|██████████| 34/34 [00:00<00:00, 94.57batch/s, acc=1, loss=2.95e-6]   \n",
      "Epoch 449: 100%|██████████| 34/34 [00:00<00:00, 54.47batch/s, acc=1, loss=2.97e-6]  \n",
      "Epoch 450: 100%|██████████| 34/34 [00:00<00:00, 65.69batch/s, acc=1, loss=2.86e-6]   \n",
      "Epoch 451: 100%|██████████| 34/34 [00:00<00:00, 64.75batch/s, acc=1, loss=2.8e-6]   \n",
      "Epoch 452: 100%|██████████| 34/34 [00:00<00:00, 112.35batch/s, acc=1, loss=2.85e-6] \n",
      "Epoch 453: 100%|██████████| 34/34 [00:00<00:00, 63.86batch/s, acc=1, loss=2.69e-6]   \n",
      "Epoch 454: 100%|██████████| 34/34 [00:00<00:00, 130.56batch/s, acc=1, loss=2.7e-6]   \n",
      "Epoch 455: 100%|██████████| 34/34 [00:00<00:00, 56.38batch/s, acc=1, loss=2.69e-6]  \n",
      "Epoch 456: 100%|██████████| 34/34 [00:00<00:00, 90.83batch/s, acc=1, loss=2.66e-6]   \n",
      "Epoch 457: 100%|██████████| 34/34 [00:00<00:00, 58.16batch/s, acc=1, loss=2.55e-6]   \n",
      "Epoch 458: 100%|██████████| 34/34 [00:00<00:00, 65.86batch/s, acc=1, loss=2.41e-6]   \n",
      "Epoch 459: 100%|██████████| 34/34 [00:00<00:00, 127.34batch/s, acc=1, loss=2.48e-6]  \n",
      "Epoch 460: 100%|██████████| 34/34 [00:00<00:00, 91.49batch/s, acc=1, loss=2.43e-6]  \n",
      "Epoch 461: 100%|██████████| 34/34 [00:00<00:00, 113.29batch/s, acc=1, loss=2.42e-6]  \n",
      "Epoch 462: 100%|██████████| 34/34 [00:00<00:00, 57.71batch/s, acc=1, loss=2.32e-6]   \n",
      "Epoch 463: 100%|██████████| 34/34 [00:00<00:00, 71.25batch/s, acc=1, loss=2.27e-6]  \n",
      "Epoch 464: 100%|██████████| 34/34 [00:00<00:00, 113.49batch/s, acc=1, loss=2.26e-6]  \n",
      "Epoch 465: 100%|██████████| 34/34 [00:00<00:00, 74.80batch/s, acc=1, loss=2.25e-6]   \n",
      "Epoch 466: 100%|██████████| 34/34 [00:00<00:00, 85.10batch/s, acc=1, loss=2.2e-6]    \n",
      "Epoch 467: 100%|██████████| 34/34 [00:00<00:00, 106.30batch/s, acc=1, loss=2.13e-6]  \n",
      "Epoch 468: 100%|██████████| 34/34 [00:00<00:00, 128.30batch/s, acc=1, loss=2.11e-6]  \n",
      "Epoch 469: 100%|██████████| 34/34 [00:00<00:00, 80.09batch/s, acc=1, loss=2.09e-6]   \n",
      "Epoch 470: 100%|██████████| 34/34 [00:00<00:00, 227.63batch/s, acc=1, loss=2.05e-6]  \n",
      "Epoch 471: 100%|██████████| 34/34 [00:00<00:00, 93.21batch/s, acc=1, loss=2.04e-6]  \n",
      "Epoch 472: 100%|██████████| 34/34 [00:00<00:00, 143.17batch/s, acc=1, loss=2e-6]     \n",
      "Epoch 473: 100%|██████████| 34/34 [00:00<00:00, 123.55batch/s, acc=1, loss=1.93e-6]  \n",
      "Epoch 474: 100%|██████████| 34/34 [00:00<00:00, 113.60batch/s, acc=1, loss=1.9e-6]   \n",
      "Epoch 475: 100%|██████████| 34/34 [00:00<00:00, 68.24batch/s, acc=1, loss=1.87e-6]   \n",
      "Epoch 476: 100%|██████████| 34/34 [00:00<00:00, 103.45batch/s, acc=1, loss=1.9e-6]   \n",
      "Epoch 477: 100%|██████████| 34/34 [00:00<00:00, 68.31batch/s, acc=1, loss=1.83e-6]   \n",
      "Epoch 478: 100%|██████████| 34/34 [00:00<00:00, 89.19batch/s, acc=1, loss=1.77e-6]   \n",
      "Epoch 479: 100%|██████████| 34/34 [00:00<00:00, 65.60batch/s, acc=1, loss=1.74e-6]   \n",
      "Epoch 480: 100%|██████████| 34/34 [00:00<00:00, 62.20batch/s, acc=1, loss=1.75e-6]   \n",
      "Epoch 481: 100%|██████████| 34/34 [00:00<00:00, 140.26batch/s, acc=1, loss=1.7e-6]   \n",
      "Epoch 482: 100%|██████████| 34/34 [00:00<00:00, 194.09batch/s, acc=1, loss=1.65e-6]  \n",
      "Epoch 483: 100%|██████████| 34/34 [00:00<00:00, 75.25batch/s, acc=1, loss=1.65e-6]  \n",
      "Epoch 484: 100%|██████████| 34/34 [00:00<00:00, 95.38batch/s, acc=1, loss=1.61e-6]   \n",
      "Epoch 485: 100%|██████████| 34/34 [00:00<00:00, 71.50batch/s, acc=1, loss=1.58e-6]   \n",
      "Epoch 486: 100%|██████████| 34/34 [00:00<00:00, 86.97batch/s, acc=1, loss=1.56e-6]   \n",
      "Epoch 487: 100%|██████████| 34/34 [00:00<00:00, 102.65batch/s, acc=1, loss=1.52e-6]  \n",
      "Epoch 488: 100%|██████████| 34/34 [00:00<00:00, 56.77batch/s, acc=1, loss=1.48e-6]   \n",
      "Epoch 489: 100%|██████████| 34/34 [00:00<00:00, 136.99batch/s, acc=1, loss=1.47e-6]  \n",
      "Epoch 490: 100%|██████████| 34/34 [00:00<00:00, 63.30batch/s, acc=1, loss=1.44e-6]   \n",
      "Epoch 491: 100%|██████████| 34/34 [00:00<00:00, 98.95batch/s, acc=1, loss=1.41e-6]   \n",
      "Epoch 492: 100%|██████████| 34/34 [00:00<00:00, 77.89batch/s, acc=1, loss=1.36e-6]   \n",
      "Epoch 493: 100%|██████████| 34/34 [00:00<00:00, 129.57batch/s, acc=1, loss=1.35e-6]  \n",
      "Epoch 494: 100%|██████████| 34/34 [00:00<00:00, 65.99batch/s, acc=1, loss=1.36e-6]   \n",
      "Epoch 495: 100%|██████████| 34/34 [00:00<00:00, 75.96batch/s, acc=1, loss=1.29e-6]   \n",
      "Epoch 496: 100%|██████████| 34/34 [00:00<00:00, 90.26batch/s, acc=1, loss=1.28e-6]  \n",
      "Epoch 497: 100%|██████████| 34/34 [00:00<00:00, 120.15batch/s, acc=1, loss=1.27e-6]  \n",
      "Epoch 498: 100%|██████████| 34/34 [00:00<00:00, 83.65batch/s, acc=1, loss=1.26e-6]   \n",
      "Epoch 499: 100%|██████████| 34/34 [00:00<00:00, 139.84batch/s, acc=1, loss=1.25e-6]  \n"
     ]
    }
   ],
   "source": [
    "clf_nl = sg_detection.NonLinearClassifier(input_size=512, hidden_size=64)\n",
    "acc = train_classifier(clf_nl, feats, stacked_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  tensor(0.7814)\n",
      "Confusion Matrix: \n",
      "[[1178    8    0]\n",
      " [   6  161    0]\n",
      " [   0    1    0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.7814),\n",
       " array([[1178,    8,    0],\n",
       "        [   6,  161,    0],\n",
       "        [   0,    1,    0]]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_performance(clf_nl, feats, stacked_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 84, 84])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10.7101,  0.0000,  3.3007,  0.0000,  0.0000,  4.7056,  6.2294,  0.0000,\n",
       "          0.7875,  0.0000,  6.0332,  9.9827,  7.6485,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  2.3347,  1.9806,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.9752,  0.0000,  0.0000,\n",
       "          5.9296,  0.0000,  0.0000,  0.0000,  0.0000,  3.1411,  1.7217,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  1.5900,  1.1487,  9.6075,  3.9059,  7.3202,\n",
       "          0.0000,  0.0000,  0.0000,  2.1230,  3.4050,  0.0000,  0.0000,  0.0000,\n",
       "          0.6814,  0.0000,  6.6914,  0.0000,  9.6066,  0.1442,  0.0000,  0.5422,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.4422,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0876,  2.1760,  0.0000,  5.8867,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  6.0191, 10.1115,  0.0000,  7.9544,  0.0000,  0.0627,\n",
       "          0.0000,  0.0000,  0.1351,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  1.0380,  0.0000,  0.0000,  0.0000,  2.5446,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  6.9066,  0.0000,  0.0000,  0.0000,  1.8484,\n",
       "          5.4561,  0.0000,  5.3564,  0.4315,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          1.5073,  0.0000,  0.0000,  0.0000,  0.6885,  0.0000,  0.0000,  9.3354,\n",
       "          1.6166,  4.3979,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, 14.4381,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  4.1243,  4.4336,  0.0000,  0.0000,  0.0000,  4.7530,\n",
       "          0.7759,  3.1051,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.5904,  4.5724,\n",
       "          0.0000,  0.0000,  0.0000,  0.4008,  0.0000,  0.0000,  0.0000,  0.8025,\n",
       "          0.0000,  0.0000,  0.0000,  0.7971,  0.0000,  5.2470,  0.0000,  0.0000,\n",
       "          0.0000,  6.2203,  4.3638,  0.0000,  0.0000, 16.6238,  1.9317, 10.7999,\n",
       "          0.0000,  0.0000,  0.0000,  5.6898,  0.0000,  0.0000,  2.4310,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  8.7514,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  1.0555,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0960,\n",
       "          0.2728, 10.1866,  0.0000,  0.0000,  0.0000,  0.0000,  1.4416,  0.0000,\n",
       "          0.0000,  0.0000,  9.3666,  1.2385,  7.6923,  0.0000,  0.0000,  3.0200,\n",
       "          0.2226,  0.4372,  0.5767,  0.0000,  0.0000,  0.0769,  0.0000,  0.0000,\n",
       "          2.5784,  0.2082,  0.0000,  0.0000,  0.0000,  0.0000, 11.0890,  0.0000,\n",
       "          0.0000,  4.1757, 10.1671,  0.0000,  0.0000,  6.3420,  4.2631, 12.0617,\n",
       "          0.0000,  0.0000,  0.0000,  3.1490,  0.3281,  0.0000,  0.0000,  3.0881,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          6.7469,  0.0000,  4.0506,  0.0000,  1.9316,  0.0000,  4.3750,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  8.8725,  0.0000, 14.2533,  6.5904,\n",
       "          2.1691,  0.0000,  4.6775,  0.0000,  0.0000,  0.0000, 10.1914,  5.1644,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.1650,  0.0000,  0.0000,\n",
       "          0.0000,  8.0267,  3.1678,  0.5859,  0.0000,  0.0000,  0.5427,  8.2548,\n",
       "          5.5621,  0.0000,  3.9862,  0.0000,  0.0000,  0.0000,  2.5414,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  8.4067,  0.0000,  0.0000,  0.0000,\n",
       "          0.6160,  2.5975,  0.0000,  0.0000,  5.1285,  0.0000,  0.0000,  3.9262,\n",
       "          4.2113,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.9315,  4.8941,\n",
       "          2.2584,  0.0000,  0.0000,  0.0000,  0.0000,  3.1024,  0.0000,  2.4152,\n",
       "          2.3184,  0.4122,  5.9922,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          6.8891,  0.0000,  0.0670,  0.1396,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          2.4390,  0.0000, 10.9242,  0.0000,  0.0000, 10.4135,  1.5106,  4.2510,\n",
       "          5.4502,  0.0000,  0.0000,  0.0000,  5.2429,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000, 18.4269,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.8026,\n",
       "          2.3588,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.9875,  0.0000,\n",
       "          0.0000,  0.0000,  5.7188,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          9.1043,  0.0000,  0.0000,  1.6431,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  4.2969,  2.7906,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000, 10.2302,  0.0000,  7.4432,  0.6271,  0.0000,  0.0000,\n",
       "          0.0000,  0.6429,  0.0000,  4.0411,  0.0000,  0.0000,  7.1182,  0.0000,\n",
       "          0.0000,  7.7650,  0.0000,  0.0000,  0.0000,  2.5939,  0.0000,  0.0000,\n",
       "          0.0000,  4.5379,  0.0000,  0.0000,  0.0000,  0.0000,  6.0067,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  2.7054,  0.0000,  0.0000,  1.5952,\n",
       "          0.0000,  0.0000,  5.3036,  0.0000,  5.0347,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  4.1141,  3.7288,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          9.1414,  0.2521,  0.0000,  0.0000, 11.3141,  0.2482,  0.0000,  0.8405,\n",
       "          1.5439,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = pre_processed_states[0, :, :, :]\n",
    "obs = obs.reshape(1, 4, 84, 84)\n",
    "obs = obs_as_tensor(obs, agent.policy.device)\n",
    "print(obs.shape)\n",
    "agent.policy.extract_features(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axes don't match array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[43mobs_to_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mobs_to_feats\u001b[0;34m(model, obss)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obs \u001b[38;5;129;01min\u001b[39;00m obss:\n\u001b[0;32m----> 5\u001b[0m         obs \u001b[38;5;241m=\u001b[39m \u001b[43mpre_process_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;66;03m# print(obs[0].shape)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoubleDQN\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/research/discovery/discovery/experiments/FeatAct_minigrid/helpers.py:88\u001b[0m, in \u001b[0;36mpre_process_obs\u001b[0;34m(obs, model)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obs\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     85\u001b[0m     obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(\n\u001b[1;32m     86\u001b[0m         obs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     87\u001b[0m     )  \u001b[38;5;66;03m# add batch dimension if its just one observation\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# bring colour channel to front\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs_as_tensor(obs, model\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Documents/research/discovery/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:655\u001b[0m, in \u001b[0;36mtranspose\u001b[0;34m(a, axes)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_transpose_dispatcher)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranspose\u001b[39m(a, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    590\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    Returns an array with axes transposed.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m \n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranspose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/research/discovery/venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: axes don't match array"
     ]
    }
   ],
   "source": [
    "feats = obs_to_feats(agent, [state[0, :, :, :]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
