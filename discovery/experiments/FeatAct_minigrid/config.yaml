env_name: 'TwoRoomEnv'
learner: 'PPO'
project_name: 'debug-minigrid'
extra_dirname: 'two_rooms_single_task_cnn'
timesteps: 1000000
policy_type: 'CnnPolicy'
cnn: 'minigrid' # 'nature', 'minigrid', 'minigrid_sp', 'minigrid_hallfeat'
# activation: 'fta'
# feat_dim: 640
activation: 'relu'
feat_dim: 32
stats_window_size: 10
record_video: True
render_mode: 'rgb_array'

# Shared hypers
lr: 0.0003 # PPO_def: 3e-4
gamma: 0.99 # PPO_def: 0.99
batch_size: 96 # PPO_def: 64, DQN_def: 32

# PPO hypers & SB3 defaults
n_epochs: 4 # 10
n_steps: 128 # 2048
n_envs: 1 # 1 #TODO this affects DDQN too!

# (D)DQN hypers & SB3 defaults
learning_starts: 1000 # 100
train_freq: 1 # 4
exploration_final_eps: 0.1 # 0.05
target_update_interval: 500 # 10000

# Env-specific hypers
random_hallway: True # TwoRoomEnv
# random_hallway: False # TwoRoomEnv
num_variants: 4 # TwoRoomEnv
